---
title: "TFM - Classification of semi-structured answers in audio"
author: "Yijia Lin, supervised by Francisco Javier Nogales Martín"
date: "`r Sys.Date()`"
output: 
  rmdformats::html_clean:
    lightbox: false
    thumbnails: false
    toc: yes
    toc_float: true
    toc_depth: 3
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
reticulate::use_python("/Users/linyijia/whisper310_env/bin/python", required = TRUE)
```

## 0. Prerequisites & Packages

To run the audio transcription and classification pipeline, the
following tools and libraries must be installed:

1.  **FFmpeg**

Install using:

```{bash}
#In Terminal (not Rmd), already done:
# brew install ffmpeg  
# if the computer is MacOS
```

FFmpeg is a command-line tool for processing audio and video files. It
is required because OpenAI Whisper depends on FFmpeg to load and convert
audio files (e.g., .mp3, .wav, .m4a) into a format suitable for
transcription. On the other hand, some other file format like .amr or
.mov could be not be read by OpenAI Whisper, and this tool can help with
format transformation. Installing it via Homebrew ensures it is
available system-wide on macOS.

<br>

2.  **The IDE of R studio and the interpreter of Python 3**

This notebook will be using both R and Python languages, so it's
important to make sure the R studio and Python 3 are installed.

These steps below are intended to ensure that Python 3.10 and the
required development libraries (e.g., zlib, llvm) are properly
installed. Creating a dedicated Python virtual environment
whisper310_env ensures that the packages used for Whisper will not
conflict with other Python environments on your system.

```{bash}
#In Terminal (not Rmd), already done:
# brew install zlib llvm
# python3.10 -m venv ~/whisper310_env
# source ~/whisper310_env/bin/activate
```

<br>

3.  **R Libraries**

Install using:

```{r, message=FALSE, warning=FALSE}
#install.packages("reticulate")
library(reticulate)
library(tidyverse)
library(readxl)
library(conflicted)
library(stringr)
#install.packages("googledrive")
library(googledrive)
library(patchwork)
library(ggrepel)
library(extrafont)
library(stringdist)
library(tidytext)
library(readr)
library(scales)
library(xml2)
library(spacyr)
library(tibble)
library(stringr)
library(gt)
library(webshot2)

#avoid conflicts in packages
conflicted::conflict_prefer("filter", "dplyr")
conflicted::conflict_prefer("select", "dplyr")
```

<br>

4.  **Python Libraries**

On macOS, especially Apple Silicon (M1/M2/M3), Whisper depends on
several low-level dynamic libraries (e.g., zlib, libc++, libunwind) for
numerical computations. These libraries are not installed or linked by
default for R + Python interoperability. If dynamic linking errors occur
(e.g., Library not loaded), you may need to manually copy these
libraries into R’s library search path like I did as shown below.

```{bash}
# No need to manually set environment variables (LDFLAGS, CPPFLAGS, DYLD_LIBRARY_PATH), unless you encounter dynamic library errors. In that case, please contact the author for troubleshooting.

# sudo cp /opt/homebrew/opt/llvm/lib/c++/libc++.1.dylib /Library/Frameworks/R.framework/Resources/lib/
# sudo cp /opt/homebrew/opt/zlib/lib/libz.1.dylib /Library/Frameworks/R.framework/Resources/lib/
# sudo cp /opt/homebrew/opt/llvm/lib/c++/libc++abi.1.dylib /Library/Frameworks/R.framework/Resources/lib/
# sudo cp /opt/homebrew/Cellar/llvm/20.1.6/lib/unwind/libunwind.1.dylib /Library/Frameworks/R.framework/Resources/lib/


```

```{r}
# No need to manually set environment variables (LDFLAGS, CPPFLAGS, DYLD_LIBRARY_PATH), unless you encounter dynamic library errors. In that case, please contact the author for troubleshooting.

# Sys.getenv("DYLD_LIBRARY_PATH")
# Sys.setenv(
#   LDFLAGS="-L/opt/homebrew/opt/zlib/lib -L/opt/homebrew/opt/llvm/lib",
#   CPPFLAGS="-I/opt/homebrew/opt/zlib/include -I/opt/homebrew/opt/llvm/include",
#   PATH="/opt/homebrew/opt/llvm/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin"
# )
```

Install using:

```{bash}
#In Terminal (not Rmd), already done:
# pip install --upgrade pip
# pip install openai-whisper pandas ffmpeg-python
```

These libraries serve the following purposes:

**openai-whisper**: Enables local speech-to-text transcription using
OpenAI's Whisper model.

**openai**: Provides access to OpenAI's API (e.g., GPT-4) to analyze and
classify open-ended responses.

**pandas**: A data analysis library used for reading Excel files,
applying classification functions row-by-row, and exporting results.

Now, we connect the r studio with the virtual python environment by
using `reticulate`:

```{r, message=FALSE, warning=FALSE, cache=TRUE}
reticulate::use_virtualenv("~/whisper310_env", required = TRUE)
py_config()
```

<br>

## 1.Data Cleaning & Processing

### 1.1 Read the data

```{r, message=FALSE, warning=FALSE}
raw <- read_excel("data/raw data/Survey data.xlsx")
```

### 1.2 Tidy and clean the data

The first step will be cleaning the data! Given the limit of structure
of Google Forms, the raw spreadsheet is not in the cleanest way, so I
will firstly try to tidy it. For any bank assigned to the respondents,
the structure of the survey was the same. Therefore, I'll try to combine
all the information in the same columns instead of having dobbled
columns.

Before that, I'll specify for each row the assigned bank according to
the random assignation:

```{r}
raw1 <- raw |>
  rename(assignation = `Asignación de marca: Selecciona una opción aleatoria para que se te asigne una marca.`) |>
  mutate(bank = case_when(
    str_detect(assignation, "◻") ~ "BBVA",
    str_detect(assignation, "◇") ~ "BBVA",
    str_detect(assignation, "○") ~ "CaixaBank",
    str_detect(assignation, "⌔") ~ "CaixaBank",
    str_detect(assignation, "⭐") ~ "CaixaBank",
    TRUE ~ NA_character_
  )) |> 
  relocate(bank, .after = assignation)
```

Then, I will process separately the data for BBVA and CaixaBank before
joining them into the main table:

```{r}
raw1 <- raw1 |> 
  filter(if_any(
    .cols = matches("^La marca de banco que se te ha asignado es: (BBVA|CaixaBank)"),
    ~ !is.na(.)
  )) |> 
  mutate(respondent_id = row_number()) |> #Create respondent ID for later joint
  relocate(respondent_id, .after = "Marca temporal")
```

```{r}
bbva_long <- raw1 |> 
  filter(bank == "BBVA") |> 
  select(respondent_id, bank,
         awareness_audio = "Por favor, graba un audio de 10 segundos (y súbelo aquí) respondiendo a la siguiente pregunta:\n\n¿Cuáles son los 3 a 5 bancos que más escuchas en tu día a día?...9"    ,
         main_audio = matches("^La marca de banco que se te ha asignado es: BBVA"),
         awareness = matches("^¿Qué nivel de reconocimiento tiene BBVA"),
         sentiment = matches("^¿Cómo es la emoción que genera la marca BBVA"),
         differentiation = matches("^¿Qué opinas sobre la diferenciación de BBVA"),
         brand_image = matches("^¿Qué opinas de la imagen de marca de BBVA")
  ) |>
  mutate(brand = "BBVA") |>
  select(respondent_id, brand,  awareness, sentiment, differentiation,brand_image, awareness_audio, main_audio)


```

```{r}
caixa_long <- raw1 |> 
  filter(bank == "CaixaBank") |> 
  select(respondent_id, bank,
         awareness_audio = "Por favor, graba un audio de 10 segundos (y súbelo aquí) respondiendo a la siguiente pregunta:\n\n¿Cuáles son los 3 a 5 bancos que más escuchas en tu día a día?...15" ,
         main_audio = matches("^La marca de banco que se te ha asignado es: CaixaBank"),
         awareness = matches("^¿Qué nivel de reconocimiento tiene CaixaBank"),
         sentiment = matches("^¿Cómo es la emoción que genera la marca CaixaBank"),
         differentiation = matches("^¿Qué opinas sobre la diferenciación de CaixaBank"),
         brand_image = matches("^Finalmente, ¿qué opinas de la imagen de marca de CaixaBank?")
  ) |>
  mutate(brand = "CaixaBank") |>
  select(respondent_id, brand,  awareness, sentiment, differentiation,brand_image, awareness_audio, main_audio)

```

```{r}
tidy <- bind_rows(bbva_long, caixa_long)
tidy_survey_data <- raw1 |> 
  select(1:8) |> 
  inner_join(tidy, by = "respondent_id") 
```

### 1.3 Download the audios in bulk

To download the audios in bulk, a Google authorization of my personal
account is required as the audios should be private and only accessible
to me.

```{r, cache=TRUE}
#drive_auth()
```

Then I prepare the list of links for downloading:

```{r, cache=TRUE}
awareness_audio<- unique(tidy_survey_data$awareness_audio)
main_audio<- unique(tidy_survey_data$main_audio)
```

Now, I download the audios and save them with ids in 2 different folders
according to the type of the audio:

```{r, message=FALSE, cache=TRUE}
# dir.create(file.path("data", "awareness_audio"), showWarnings = FALSE)
# dir.create(file.path("data", "main_audio"), showWarnings = FALSE)
# 
# for (i in seq_along(main_audio)) {
#   file_url <- main_audio[i]
#   try({
#     drive_file <- drive_get(as_id(file_url))
#     
#     # get the original file extension
#     original_name <- drive_file$name
#     ext <- tools::file_ext(original_name)
#     
#     # generate the final file path
#     output_path <- file.path("data", "main_audio", paste0("audio_", i, ".", ext))
#     
#     # download the audio
#     drive_download(drive_file, path = output_path, overwrite = TRUE)
#   }, silent = TRUE)
# }
# 
# for (i in seq_along(awareness_audio)) {
#   file_url <- awareness_audio[i]
#   try({
#     drive_file <- drive_get(as_id(file_url))
#     
#     # get the original file extension
#     original_name <- drive_file$name
#     ext <- tools::file_ext(original_name)
#     
#     # generate the final file path
#     output_path <- file.path("data", "awareness_audio", paste0("audio_", i, ".", ext))
#     
#     # download the audio
#     drive_download(drive_file, path = output_path, overwrite = TRUE)
#   }, silent = TRUE)
# }
```

### 1.4 Convert audios into text locally using OpenAI Whisper

#### 1.4.1 Make sure the audio formats are compatible with OpenAI Whisper

Firstly, I need to convert some special formats of audios into formats
which are compatibles with OpenAI Whisper.

```{r, cache=TRUE}
# Create a new folder for saving the converted audios
# dir.create(file.path("data", "processed_main_audio"), showWarnings = FALSE)
# dir.create(file.path("data", "processed_awareness_audio"), showWarnings = FALSE)
```

```{r, cache=TRUE, message=FALSE, warning=FALSE}
# Set folder with original audio files
input_folder <- "data/main_audio"  

# Set folder where processed files will be saved
output_folder <- "data/processed_main_audio"
dir.create(output_folder, showWarnings = FALSE)

# Get all file paths in the input folder
files <- list.files(input_folder, full.names = TRUE)


# Loop through each file
for (f in files) {
  # Get file extension (case-insensitive)
  ext <- tools::file_ext(f)
  
  # Construct output file path (use .wav if converting)
  base_name <- tools::file_path_sans_ext(basename(f))
  
  # If incompatible format (amr or mov), convert to .wav
  if (tolower(ext) %in% c("amr", "mov")) {
    output_file <- file.path(output_folder, paste0(base_name, ".wav"))
    system(sprintf('ffmpeg -y -i "%s" "%s"', f, output_file))
  } else {
    # If compatible format, just copy the file as is
    output_file <- file.path(output_folder, basename(f))
    file.copy(f, output_file, overwrite = TRUE)
  }
}
```

```{r, message=FALSE, warning=FALSE, cache=TRUE}
# # Repeat the same process
# input_folder <- "data/awareness_audio"  
# 
# # Set folder where processed files will be saved
# output_folder <- "data/processed_awareness_audio"
# dir.create(output_folder, showWarnings = FALSE)
# 
# # Get all file paths in the input folder
# files <- list.files(input_folder, full.names = TRUE)
# 
# 
# # Loop through each file
# for (f in files) {
#   # Get file extension (case-insensitive)
#   ext <- tools::file_ext(f)
#   
#   # Construct output file path (use .wav if converting)
#   base_name <- tools::file_path_sans_ext(basename(f))
#   
#   # If incompatible format (amr or mov), convert to .wav
#   if (tolower(ext) %in% c("amr", "mov")) {
#     output_file <- file.path(output_folder, paste0(base_name, ".wav"))
#     system(sprintf('ffmpeg -y -i "%s" "%s"', f, output_file))
#   } else {
#     # If compatible format, just copy the file as is
#     output_file <- file.path(output_folder, basename(f))
#     file.copy(f, output_file, overwrite = TRUE)
#   }
# }
```

#### 1.4.2 Convert the audios

```{python, cache=TRUE}
# import ssl
# ssl._create_default_https_context = ssl._create_unverified_context #solve certification problems, as here we are doing python in r studio
# 
# import whisper
# model = whisper.load_model("large")
# 
# import os  
# import pandas as pd
# 
# # Set the folder path that contains the audio files
# AUDIO_FOLDER = "data/processed_main_audio"
# 
# 
# # Supported audio formats (incompatible ones will be skipped)
# AUDIO_EXTENSIONS = [".mp3", ".wav", ".m4a", ".mp4", ".webm", ".ogg", ".flac"]
# 
# # List to store results: filename and its transcript
# results = []
# 
# # Loop through all audio files in the folder
# for filename in os.listdir(AUDIO_FOLDER):
#     if any(filename.endswith(ext) for ext in AUDIO_EXTENSIONS):
#         audio_path = os.path.join(AUDIO_FOLDER, filename)
#         print(f"Transcribiendo: {filename}...")  
# 
#         # Transcribe audio to text with Whisper
#         result = model.transcribe(audio_path, language="Spanish")
#         transcript = result["text"]  # Get the transcribed text
# 
#         # Add result to list (filename + transcript)
#         results.append({
#             "file": filename,
#             "transcript": transcript
#         })
# 
# # Save all transcripts into a single CSV file for analysis or R
# df = pd.DataFrame(results)
# csv_path = os.path.join(AUDIO_FOLDER, "transcripts_main.csv")
# df.to_csv(csv_path, index=False, encoding="utf-8")
# 
# print("¡Transcripción completada y guardada en transcripts.csv!")

# Note:
# If you see the warning:
# "UserWarning: FP16 is not supported on CPU; using FP32 instead"
# This is expected. Whisper automatically switches from half precision (FP16) to single precision (FP32) on CPU-only devices. It does not affect the transcription results but may slightly impact performance.
```

```{python, cache=TRUE, message=FALSE, warning=FALSE}
# # Repeat the same processes for awareness audios
# AUDIO_FOLDER = "data/processed_awareness_audio"
# 
# # Supported audio formats (incompatible ones will be skipped)
# AUDIO_EXTENSIONS = [".mp3", ".wav", ".m4a", ".mp4", ".webm", ".ogg", ".flac"]
# 
# # List to store results: filename and its transcript
# results = []
# 
# # Loop through all audio files in the folder
# for filename in os.listdir(AUDIO_FOLDER):
#     if any(filename.endswith(ext) for ext in AUDIO_EXTENSIONS):
#         audio_path = os.path.join(AUDIO_FOLDER, filename)
#         print(f"Transcribiendo: {filename}...")  
# 
#         # Transcribe audio to text with Whisper
#         result = model.transcribe(audio_path, language="Spanish")
#         transcript = result["text"]  # Get the transcribed text
# 
#         # Add result to list (filename + transcript)
#         results.append({
#             "file": filename,
#             "transcript": transcript
#         })
# 
# # Save all transcripts into a single CSV file for analysis or R
# df = pd.DataFrame(results)
# csv_path = os.path.join(AUDIO_FOLDER, "transcripts_awareness.csv")
# df.to_csv(csv_path, index=False, encoding="utf-8")
# 
# print("¡Transcripción completada y guardada en transcripts.csv!")
```

After converting the audios into texts (saved in csv format), now we
read the csv files to access them in R studio:

```{r, cache=TRUE}
transcrip_main <- read.csv("data/processed_main_audio/transcripts_main.csv")
transcrip_awareness <- read.csv("data/processed_awareness_audio/transcripts_awareness.csv")
```

### 1.4.3 Join and clean the transcripts

In case there are audios not conform to requirements, I'll do a further
cleaning: filtering out the transcripts which contains less than 20
words (as they're supposed to be 1 min long).

```{r, cache=TRUE}
transcript<- transcrip_main |> 
  left_join(transcrip_awareness, by = "file") |> 
  mutate(respondent_id= as.integer(sub("^audio_(\\d+)\\..*$", "\\1", file))) |> 
  rename(awareness_audio=transcript.y,
         main_audio=transcript.x) |> 
  mutate(
    clean_text = str_remove_all(main_audio, "[[:punct:]]"),  
    word_count = str_count(clean_text, "\\S+")                
  ) |> 
  filter(word_count >= 20) |> 
  select(respondent_id,awareness_audio,main_audio)
```

```{r, cache=TRUE}
tidy_join_transcript <- tidy_survey_data |> 
  select(-c("awareness_audio","main_audio")) |> 
  inner_join(transcript, by = "respondent_id") |> 
  rename(CCAA="¿En cuál de las siguientes Comunidades/Ciudades Autónomas vives?")
```

### 1.5 Balance check

I'll first check if the random assignation gave a fair split of
observations for each of the banks:

```{r, message=FALSE, warning=FALSE}
#set the same theme for all plots later
loadfonts(device = "pdf") #macOs
my_theme <- theme_void() +
  theme(
    legend.position = "right",
    legend.title = element_text(size = 10, face = "bold", family = "Times New Roman"),
    legend.text = element_text(size = 9, family = "Times New Roman"),
    plot.title = element_text(
      size = 12,
      face = "bold",
      hjust = 0.5,
      family = "Times New Roman",
      margin = margin(b = 10)
    ),
    plot.caption = element_text(
      size = 8,
      family = "Times New Roman",
      hjust = 1,
      color = "gray40"
    ),
    plot.margin = margin(t = 20, r = 40, b = 20, l = 40),
    legend.box.margin = margin(l = 10),
    text = element_text(family = "Times New Roman")
  )

# create the function to save the plots in the same format
fig_dir <- "figures"
if (!dir.exists(fig_dir)) {
  dir.create(fig_dir)
}

save_my_plot <- function(p, filename) {
  ggsave(file.path(fig_dir, filename),
          plot = p, dpi = 600, width = 9, height = 5, units = "in")
}
```

```{r brand balance, cache=TRUE}
freq_bank <- tidy_join_transcript |> 
  count(brand) |> 
  mutate(
    percent = n / sum(n),
    label = scales::percent(percent, accuracy = 0.1)  
  )


p1 <- ggplot(freq_bank, aes(x = "", y = percent, fill = brand)) +
  geom_bar(stat = "identity", width = 1, color = "white", linewidth = 0.3) +
  coord_polar("y", start = 0) +
  ggrepel::geom_text_repel(
    aes(label = label),
    position = position_stack(vjust = 0.5),
    size = 5,
    color = "white",  
    point.padding = 0.2,
    min.segment.length = 0,  
    segment.color = "gray30",
    segment.size = 0.3,
    show.legend = FALSE
  ) +
  labs(
    title = "Distribution of Audios between BBVA and CaixaBank",
    fill = "Bank Brand",
    caption = "Source: Yijia's analysis"
  ) +
  scale_fill_manual(
    values = c("#1f77b4", "#ff7f0e"),  
    labels = function(x) tools::toTitleCase(tolower(x)) 
  ) +
  my_theme +
  theme(
    plot.caption = element_text(
      size = 9,
      hjust = 1,
      family = "Times New Roman",
      face = "italic",
      margin = margin(t = 10)
    )
  )
p1
```

```{r, cache=TRUE}
save_my_plot(p1, "figure1_bank_distribution.png")
```

According to the pie plot, I have balanced observations for both brands
(19 observations of each).

Now, I would like to check the distribution among the social-economic
variables, although the sample itself is not representative and then I
didn't expect a very balanced sample.

```{r socio-economic balance, cache=TRUE}
create_pie_chart <- function(data, variable, title) {

  freq_data <- data |> 
    count({{variable}}) |> 
    drop_na() |> 
    mutate(
      percent = n / sum(n),
      label_raw = paste0(scales::percent(percent, accuracy = 1), "\n", as.character({{variable}}))
    ) |> 
    arrange(desc(percent)) |> 
    mutate(label = ifelse(row_number() <= 3, label_raw, NA))  #only show labels for the top 3

  ggplot(freq_data, aes(x = "", y = percent, fill = {{variable}})) +
    geom_bar(stat = "identity", width = 1, color = "white") +
    coord_polar("y", start = 0) +
    geom_text(
      aes(label = label),
      position = position_stack(vjust = 0.5),
      size = 2.5,
      lineheight = 0.9,
      color = "black",
      show.legend = FALSE
    ) +
    labs(title = title) +
    theme_void() +
    theme(
      plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
      plot.margin = margin(5, 5, 5, 5),
      legend.position = "none"
    )
}



p1 <- create_pie_chart(tidy_join_transcript, Sexo, "Sexo")
p2 <- create_pie_chart(tidy_join_transcript, Edad, "Edad")
p3 <- create_pie_chart(tidy_join_transcript, Estudios, "Estudios")
p4 <- create_pie_chart(tidy_join_transcript, `Ingresos mensuales brutos del hogar`, "Ingresos del Hogar")
p5 <- create_pie_chart(tidy_join_transcript, `Clase Social`, "Clase Social")
p6 <- create_pie_chart(tidy_join_transcript, CCAA, "CCAA")


combined_plot <- (p1 + p2 + p3) / (p4 + p5 + p6) +
  plot_annotation(
    title = "Socialeconomic Variables Balance Check",
    caption = "Source: Yijia's analysis",
    theme = theme(
      plot.title = element_text(
        size = 14,
        face = "bold",
        hjust = 0.5,
        family = "Times New Roman",
        margin = margin(b = 10)
      ),
      plot.caption = element_text(        
        size = 9,
        hjust = 1,
        family = "Times New Roman",
        face = "italic",
        margin = margin(t = 10)
      )
    )
  )


combined_plot
```

```{r, cache=TRUE}
save_my_plot(combined_plot,"figure2_socialeconomic_variables_balance_check.png")
```

It's observed that only sexes are more or less balanced (58% male \~ 42%
female), but for other variables, it's not balanced: most respondents of
this survey have an age of 18-27 years old, they've been educated in
universities, the monthly house income is typically less than 2000€,
most belongs to medium social class, and live in Madrid.

However, the inblance in the sample doesn't matter in this research, as
I only try to investigate the accuracy of this methodology itself but
not intend to generalized the conclusions.

<br>

## 2. Baseline model: Text Mining + BERT

Before introducing LLMs to generate the indexes, I'll firstly try to
create a baseline model for each of these indexes by text mining tools
and BERT, with the purpose of setting a minimum expectation of accuracy
for LLM models.

### 2.1 Awareness index

```{r, cache=TRUE}
# create a brand dictionary（as Whisper usually makes mistakes especially while detecting brand names）
bank_aliases <- list(
  "BBVA" = c("BBVA", "BVA", "PPVA", "BVBA", "VVA"),
  "CaixaBank" = c("CaixaBank", "CaxaBank", "Cajabank", "Caixabán", "La Caixa", "la caixa", "Caixa", "La caixa", "La CaixaBank"),
  "Santander" = c("Santander", "Sandander", "Sanander", "San Ander", "Banco Santander", "Banc Santander"),
  "Sabadell" = c("Sabadell", "Banco Sabadell", "Sabadel", "Savadell"),
  "Ibercaja" = c("Ibercaja", "Hipercaja", "Vercaja"),
  "ING" = c("ING", "ING Direct", "ING Directa"),
  "Revolut" = c("Revolut"),
  "Openbank" = c("Open Bank", "Openbank"),
  "Unicaja" = c("Unicaja"),
  "Liberbank" = c("LiberBank", "Liberbank"),
  "Caja Rural" = c("Caja Rural"),
  "Imagine" = c("Imagine"),
  "Deutsche Bank" = c("Deutsche Bank"),
  "Evo" = c("Evo"),
  "Trade Republic" = c("Trade Republic"),
  "NG" = c("NG", "NG Direct", "NG."),
  "Cajamar" = c("Cajamar"),
  "Caixa Popular" = c("Caixa Popular"),
  "Banco Mars" = c("Banco Mars"),
  "Bankinter" = c("Bankinter", "Van Ginder", "Banquinter", "Bunkinter", "Bank inter")
)
```

```{r, cache=TRUE}
# clean the sentences which repeat the question
# to detect correctly 
clean_awareness_text <- function(text) {
  gsub(
    pattern = "(?i)(los bancos que más.*?(son|serían|pueden ser|diría que|han sido|yo creo que|suelo escuchar|en mi día a día)[,\\s]*)",
    replacement = "",
    x = text,
    perl = TRUE
  )
}

# create the function to generate the Awareness Index（1–7）
compute_awareness_score_scaled <- function(text, assigned_bank, bank_aliases) {
  if (is.na(text) || is.na(assigned_bank)) return(NA)

  # Step 1: clean the sentences
  cleaned_text <- clean_awareness_text(text)

  # Step 2: tokenization
  tokens <- unlist(strsplit(cleaned_text, ",|\\s+"))
  tokens <- tolower(trimws(tokens))

  # Step 3: fuzzy assignation of brands
  aliases <- tolower(bank_aliases[[assigned_bank]])
  match_scores <- sapply(tokens, function(tok) {
    min(stringdist(tok, aliases, method = "jw"))
  })

  threshold <- 0.2
  hit_positions <- which(match_scores < threshold)

  # Step 4: 1 for not mentioning the assigned brand
  if (length(hit_positions) == 0) return(1)

  # Step 5: find the firstly mentioned brand (and the first 5)
  pos <- hit_positions[1]
  if (pos > 5) pos <- 5

  # Step 6: scaling to 1-7（pos 1–5 → 1–7）
  raw_score <- max(0, 1 - 0.2 * (pos - 1))  # e.g. 1.0 → 0.8 → ... → 0.2
  scaled_score <- 1 + 6 * raw_score        # scaling to 1–7
  return(round(scaled_score))              # round the number to integers in the Likert scale
}
```

```{r, cache=TRUE}
tidy_join_transcript1 <- tidy_join_transcript  |> 
  rowwise()  |> 
  mutate(
    awareness_index_base = compute_awareness_score_scaled(
      awareness_audio,
      brand,
      bank_aliases
    )
  )  |> 
  ungroup()  
```

### 2.2 Sentiment index

I'll use text mining tools to conduct the sentiment analysis, but first
of all, I need to download a Spanish sentiment lexicon as R libraries
doesn't have it in Spanish. The lexicon dictionary I use is from
IULA-UPF ([Institut de Lingüística Aplicada -
Barcelona](https://www.upf.edu/web/iula)), and the link to download the
lexicon is
<https://github.com/ITALIC-US/ML-Senticon/blob/main/senticon.es.xml>.

```{r, cache=TRUE}
xml_file <- read_xml("data/lexicons/senticon.es.xml")
lemmas <- xml_find_all(xml_file, ".//lemma")
senticon_es <- map_dfr(lemmas, function(node) {
  word <- xml_text(node) %>% str_trim()
  polarity <- xml_attr(node, "pol") %>% as.numeric()

  tibble(word = str_to_lower(word), polarity = polarity)
}) %>%
  filter(!is.na(polarity)) %>%
  distinct(word, .keep_all = TRUE)
```

I'll use `spacyr` to tokenize and lemmatize the transcripts (main audio)
in Spanish:

```{bash}
# pip install spacy
# python -m spacy download es_core_news_md
```

```{r}
if (!dir.exists("data/intermediate")) dir.create("data/intermediate", recursive = TRUE)
readr::write_csv(tidy_join_transcript1, "data/intermediate/tidy_join_transcript1.csv")
```

```{python, cache=TRUE}
import spacy
import pandas as pd

# model the Spanish model
nlp = spacy.load("es_core_news_md")


df = pd.read_csv("data/intermediate/tidy_join_transcript1.csv")

results = []

for idx, row in df.iterrows():
    respondent_id = row["respondent_id"]
    doc = nlp(str(row["main_audio"]))
    for token in doc:
        if not token.is_punct and not token.is_space:
            results.append({
                "respondent_id": respondent_id,
                "lemma": token.lemma_.lower()
            })

# save as dataframe
lemmas_df = pd.DataFrame(results)
lemmas_df.to_csv("data/intermediate/lemmas.csv", index=False)
```

Then, I apply the lexicons dictionary to the transcripts by
`inner_join`:

```{r, cache=TRUE}
tokens_lemma <- readr::read_csv("data/intermediate/lemmas.csv")
sentiment_scores <- tokens_lemma %>%
  inner_join(senticon_es, by = c("lemma" = "word")) %>%
  group_by(respondent_id) %>%
  summarise(sentiment_raw = mean(polarity, na.rm = TRUE)) %>%
  mutate(sentiment_index = round(scales::rescale(sentiment_raw, to = c(1, 7))))

```

Finally, I join the sentiment scores to the original tibble.

```{r, cache=TRUE}
tidy_join_transcript2 <- tidy_join_transcript1 |> 
  left_join(sentiment_scores, by="respondent_id") |> 
  select(-"sentiment_raw") |> 
  rename(sentiment_index_base=sentiment_index)
```

### 2.3 Differentiation index

To compute the Differentiation Index, I employed a semantic similarity
approach using a multilingual BERT model. As reference prompts, I
designed a set of short, general-purpose sentences that semantically
express the concept of brand distinctiveness without assuming any
specific source of differentiation (e.g., technology, customer focus,
pricing). This design choice ensures compatibility with a wide range of
respondent expressions while maintaining methodological neutrality.
Example prompts include: “Este banco es diferente a los demás.”, “Tiene
algo único que lo distingue del resto.”, and “No es un banco
convencional.” These prompts were embedded using Sentence-BERT and used
to compute cosine similarity with each respondent’s free-form
description of the assigned brand.

Firstly, I install the `sentence-transformers` python package.

```{bash, cache=TRUE}
# pip install sentence-transformers
```

Then, I prepared the input and the prompts for the BERT model:

```{r, cache=TRUE}
readr::write_csv(tidy_join_transcript2, "data/intermediate/tidy_join_transcript2.csv")
```

```{python, eval=FALSE, echo=TRUE}
# # bert_differentiation.py
# 
# from sentence_transformers import SentenceTransformer, util
# import pandas as pd
# import numpy as np
# 
# # 1. Load the input CSV from R
# df = pd.read_csv("data/intermediate/tidy_join_transcript2.csv")  # adapt path if needed
# texts = df["main_audio"].astype(str).tolist()
# 
# # 2. Define general-purpose differentiation prompts (in Spanish)
# prompts = [
#     "Este banco es diferente a los demás.",
#     "Tiene algo único que lo distingue del resto.",
#     "Se nota que no es como otros bancos.",
#     "Ofrece una propuesta distinta a lo habitual.",
#     "No es un banco convencional.",
#     "Tiene una manera propia de hacer las cosas.",
#     "Es claramente un banco con enfoque diferente."
# ]
# 
# # 3. Load the multilingual BERT model
# model = SentenceTransformer("distiluse-base-multilingual-cased-v2")
# 
# # 4. Encode texts and prompts
# text_embeddings = model.encode(texts, convert_to_tensor=True)
# prompt_embeddings = model.encode(prompts, convert_to_tensor=True)
# 
# # 5. Compute cosine similarity
# similarity_matrix = util.cos_sim(text_embeddings, prompt_embeddings).cpu().numpy()
# 
# # 6. Take the maximum similarity score for each respondent
# max_similarities = np.max(similarity_matrix, axis=1)
# 
# # 7. Rescale to 1–7 differentiation index
# scaled_scores = 1 + 6 * (max_similarities - np.min(max_similarities)) / (np.max(max_similarities) - np.min(max_similarities))
# df["differentiation_index"] = np.round(scaled_scores)
# 
# # 8. Save output for R
# df[["respondent_id", "differentiation_index"]].to_csv("data/intermediate/differentiation_index.csv", index=False)
# 
# print("✅ Differentiation index saved to data/intermediate/differentiation_index.csv")
```

As this model is computationally costly, and the R studio burnt out
several times while running the codes above, I will not run this part of
Python code here, but to save it as .py and run it in the Terminal:

```{bash}
# conda create -n tfm-bert python=3.10
# conda activate tfm-bert
# pip install sentence-transformers pandas numpy
# cd ~/Desktop/MCSC/TFM/Project/Feature-Extraction-and-Brand-Index-Creation-from-Semi-Structured-Spoken-Responses-using-LLMs-and-ML-tools
# python data/intermediate/bert_differentiation.py
```

```{r, cache=TRUE}
# Join the scores to the main tibble
diff_scores <- readr::read_csv("data/intermediate/differentiation_index.csv")

tidy_join_transcript3 <- tidy_join_transcript2 %>%
  left_join(diff_scores, by = "respondent_id") |> 
  rename(differentiation_index_base=differentiation_index)

readr::write_csv(tidy_join_transcript3, "data/intermediate/tidy_join_transcript3.csv")
```

### 2.4 Performance evaluation

To evaluate the performance of the proposed index predictions—derived
from both rule-based methods and LLM prompt outputs—against
human-annotated scores on a 1–7 Likert scale, we selected **three
complementary evaluation metrics**: **Mean Absolute Error (MAE)**,
**Spearman’s rank correlation coefficient (ρ)**, and **1-off Accuracy**.
Each metric captures a different and relevant dimension of prediction
quality in the context of ordinal, human-rated scores.

**1. Mean Absolute Error (MAE)**

MAE was chosen as the **primary metric for measuring prediction
accuracy**. It quantifies the average absolute deviation between model
predictions and human ratings.

This metric is:

-   **Scale-aware**, making it suitable for Likert-type ratings (1–7);

-   **Interpretation-friendly**, as it reflects how many points the
    model typically deviates from the ground truth;

-   **Robust to outliers** compared to squared-error metrics such as
    MSE, which are less appropriate for short, bounded rating scales.

Given the limited sample size (n = 29), MAE provides a **stable and
intuitive measure of absolute prediction quality**.

**2. Spearman’s Rank Correlation Coefficient (ρ)**

Spearman’s ρ was selected to assess the **ordinal consistency** between
predicted scores and human annotations. As a non-parametric rank-based
metric, it evaluates whether the predicted scores preserve the
**relative ordering** of responses, independent of their exact values.

This is especially relevant because:

-   Likert scales are **ordinal by nature**, and rank alignment may
    matter more than exact numerical matching;

-   It is **less sensitive to score compression or skew**—common in
    subjective human ratings;

-   It is particularly **appropriate for small samples**, where
    assumptions of linearity or normality (required by Pearson’s r) may
    not hold.

Thus, Spearman ρ serves as a complementary metric to MAE, providing
insights into **directional alignment** between models and humans.

**3. 1-off Accuracy**

This metric captures the **practical tolerance** of the prediction task
by measuring the proportion of predictions that fall within ±1 point of
the true human rating.

Its inclusion reflects:

-   The **realistic fuzziness** inherent in subjective annotations;

-   The fact that a predicted 5 vs an annotated 6 is **not a critical
    error** in most qualitative applications;

-   The need to quantify the model’s **"acceptable correctness"** beyond
    rigid exact-match accuracy.

1-off Accuracy offers an interpretable, **task-relevant performance
measure**, especially for user-perception data where perfect alignment
is neither expected nor necessary.

```{r, cache=TRUE}

# Create a function to standarize the evaluation
evaluate_model <- function(pred, truth) {
  mae <- mean(abs(pred - truth), na.rm = TRUE)
  spearman <- suppressWarnings(cor(pred, truth, method = "spearman", use = "pairwise.complete.obs"))
  one_off <- mean(abs(pred - truth) <= 1, na.rm = TRUE)

  return(c(
    MAE = round(mae, 3),
    Spearman = round(spearman, 3),
    OneOffAcc = round(one_off * 100, 1)  
  ))
}

results_base <- tibble::tibble(
  Index = character(),
  MAE = numeric(),
  Spearman = numeric(),
  OneOffAcc = numeric()
)

indexes <- c("awareness", "sentiment", "differentiation")

# go through all indexes, and find their _index_base 
tidy_join_transcript3 <- readr::read_csv("data/intermediate/tidy_join_transcript3.csv")

for (ix in indexes) {
  pred_col <- paste0(ix, "_index_base")
  truth_col <- ix  
  
  eval_result <- evaluate_model(tidy_join_transcript3[[pred_col]], tidy_join_transcript3[[truth_col]])
  
  results_base <- dplyr::add_row(results_base, Index = ix, !!!as.list(eval_result))
}

```

```{r}

results_base_gt <- results_base %>%
  gt(rowname_col = "Index") %>%
  fmt_number(columns = c(MAE, Spearman, OneOffAcc), decimals = 3) %>%
  fmt_percent(columns = OneOffAcc, scale_values = FALSE, decimals = 1) %>%
  tab_header(
    title = md("**Baseline Model Evaluation**"),
    subtitle = "Evaluation of the base index model compared to manual annotations"
  ) %>%
  cols_label(
    MAE = "MAE",
    Spearman = "Spearman ρ",
    OneOffAcc = "One-Off Accuracy (%)"
  ) %>%
  tab_source_note(
    source_note = "Source: Yijia's analysis based on survey data (N = 38)"
  ) %>%
  tab_options(
    table.font.size = "small",
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    row.striping.include_table_body = TRUE
  )

results_base_gt

save_my_table <- function(tbl, filename) {
  gtsave(tbl, filename = file.path(fig_dir, filename))
}

save_my_table(results_base_gt,  "figure3_baseline_model_evaluation.png")
```

The performance of the baseline models across the three brand perception
dimensions is limited:

-   **Awareness**:\
    High prediction error (MAE = 2.90), weak rank correlation (ρ =
    0.21), and very low 1-off accuracy (31.6%) suggest poor alignment
    with human ratings. The rule-based method likely lacks precision in
    detecting brand mentions and ordering relevance.

-   **Sentiment**: Moderate performance (MAE = 1.37, ρ = 0.17, 1-off =
    60.5%). While not highly accurate, this model captures some degree
    of emotional polarity and could serve as a viable baseline for
    future LLM comparisons.

-   **Differentiation**:\
    Poor performance, with negative rank correlation (ρ = –0.51) and
    high MAE (1.90). This suggests that the semantic similarity method
    used in the BERT-based baseline may not effectively capture
    perceived brand uniqueness.

> Among the three dimensions, only the sentiment baseline shows
> marginally acceptable performance. Awareness and differentiation
> models will likely require redesign or replacement using more robust
> methods (e.g. LLMs or hybrid pipelines).

## 3. Selection of LLM for Evaluation

In order to evaluate the effectiveness of automatic analysis on
Spanish-language audio transcriptions, we selected three distinct Large
Language Models (LLMs) based on their current performance in academic
benchmarks, multilingual capabilities, cost-efficiency, and
architectural diversity. The selection is grounded in recent comparative
research and official technical documentation.

First, **GPT-4o**, developed by OpenAI, was chosen as the primary model.
It has consistently ranked at the top of evaluations such as the Chatbot
Arena and the MT-Bench for its superior capabilities in complex
reasoning, structured output, and instruction following. Most notably,
it demonstrates strong multilingual performance, including for Spanish,
as reported in the GPT-4o System Card published by OpenAI (2024).
Moreover, recent studies indicate that GPT-4o displays notable
robustness to prompt variation, minimizing the need for excessive prompt
engineering in practical applications (Zhao et al., 2023). These
attributes make GPT-4o a strong candidate for our baseline model.

To introduce architectural diversity and cross-validation, we also
included **Claude 3 Sonnet**, developed by Anthropic. This model has
emerged as a serious contender to GPT-4o, with highly competitive
results in structured response generation tasks. It is particularly
suited for evaluations requiring consistent JSON-formatted outputs, a
core requirement in this research. Furthermore, Claude 3 Sonnet offers a
balanced trade-off between quality and cost, making it especially
suitable for batch processing in research environments. Technical
documentation from Anthropic (2024) affirms its capabilities in both
English and Spanish, supporting its relevance to our multilingual
dataset.

The third model incorporated into the evaluation is **Mistral 7B**, a
leading open-source alternative. Despite its smaller size, Mistral 7B
has outperformed earlier open-source models such as LLaMA 2 in zero-shot
and few-shot benchmarks. Running this model locally not only ensures
cost control—eliminating per-token API expenses—but also enhances
transparency and reproducibility. Including Mistral 7B also allows us to
investigate whether closed-source commercial models consistently
outperform open alternatives in tasks involving informal spoken Spanish,
a consideration highlighted in recent comparative studies (Zhao et al.,
2024).

Other models, such as **Google Gemini 1.5 Pro**, were considered but
ultimately excluded due to limited evidence of superior Spanish language
performance. Additionally, models like **Cohere's Command R+**, while
excellent in retrieval-augmented generation (RAG), are not optimized for
subjective classification tasks without retrieval support. Lastly,
**LLaMA 2-7B** was excluded based on its substantially lower benchmark
performance across reasoning tasks when compared to both GPT-4o and
Claude 3.

This tripartite model selection—combining a top-tier commercial model
(GPT-4o), a competitive alternative from a different provider (Claude 3
Sonnet), and a state-of-the-art open-source model (Mistral 7B)—ensures
that our evaluation captures not only accuracy and robustness but also
cost considerations and implementation feasibility in both academic and
applied settings.

## 4. LLM Models: **GPT 4o, **Claude 3 Sonnet and Mistral 7B**

### 4.1 Load APIs' keys

```{r}
readRenviron(".Renviron")
openai_key <- Sys.getenv("OPENAI_API_KEY")
anthropic_key <- Sys.getenv("ANTHROPIC_API_KEY")
mistral_key <- Sys.getenv("MISTRAL_API_KEY")
```

### 4.2 Set some parameters

For consistency across models, the `temperature` parameter was set to
**0.3** for all LLMs (GPT-4o, Claude 3 Sonnet, and Mistral 7B). While
each API may support slightly different ranges (e.g., OpenAI up to 2.0,
Anthropic and Mistral typically up to 1.0), the meaning of the
temperature setting remains consistent: lower values yield **more stable
and deterministic outputs**, which is essential for our structured
scoring task. This design follows best practices recommended in the
documentation of each provider (OpenAI, 2023; Anthropic, 2024; Mistral,
2024).

And `time.sleep` was set to 1.5 seconds to prevent exceeding rate
limits.

### 4.3 Prompt Engineering Strategies

To enable reliable extraction of brand perception indices from
semi-structured spoken responses, this study adopts a multi-stage
**Prompt Engineering Strategy** tailored to the capabilities and
limitations of Large Language Models (LLMs). The goal is to guide LLMs
in generating structured and interpretable outputs aligned with
human-annotated brand indices. The full pipeline consists of four
iterative stages: **Direct Prompting, Few-shot Prompting,
Chain-of-Thought Prompting, and Prompt Generation**, each building upon
the previous to improve performance and alignment.

#### **Stage v1: Direct Prompting (Zero-shot)**

The initial stage uses task-oriented, minimal prompts without examples.
These prompts directly instruct the LLM to assign brand scores (e.g.,
awareness, sentiment, differentiation) based on input responses,
establishing a **baseline** for performance and identifying the model’s
default behavior.

```{python}
prompt_v1 = (
    "Analiza el siguiente testimonio hablado en dos partes. "
    "Evalúa la percepción de marca del hablante asignando un valor numérico entre 1 y 7 en las siguientes dimensiones: awareness, sentiment, differentiation y overall.\n\n"
    
    "La marca que se le ha asignado al hablante es: {brand}.\n\n"

    "Contexto de las preguntas realizadas al hablante:\n"
    "Fragmento 1 (awareness_audio) corresponde a la siguiente pregunta:\n"
    "> “¿Cuáles son los 3 a 5 bancos que más escuchas en tu día a día?”\n"
    "Fragmento 2 (main_audio) corresponde a las siguientes tres preguntas sobre la marca asignada ({brand}):\n"
    "> 1. ¿Qué te hace sentir {brand}?\n"
    "> 2. ¿En qué aspectos crees que {brand} se diferencia de otras compañías bancarias?\n"
    "> 3. Finalmente, ¿qué opinas de la imagen de marca de {brand}?\n\n"

    "Fragmento 1 (awareness_audio):\n"
    "\"{awareness_audio}\"\n\n"
    "Fragmento 2 (main_audio):\n"
    "\"{main_audio}\"\n\n"

    "Formato de salida (JSON):\n"
    "{{\n"
    "  \"respondent_id\": \"{respondent_id}\",\n"
    "  \"awareness\": valor,\n"
    "  \"sentiment\": valor,\n"
    "  \"differentiation\": valor,\n"
    "  \"overall\": valor\n"
    "}}\n\n"

    "Devuelve solo el JSON, sin explicaciones adicionales."
)
```

#### **Stage v2: Few-shot Prompting**

This stage incorporates **manually annotated examples** into the prompt,
allowing the LLM to learn the desired task behavior from prior cases. By
leveraging **in-context learning** (Brown et al., 2020), the model is
guided to generalize from labeled examples to new inputs.

Few-shot prompting has been shown to dramatically enhance LLM
performance on classification and reasoning tasks with limited data
(Brown et al., 2020).

To construct the few-shot prompt examples, we selected two
representative responses based on their overall index scores: one at the
25th percentile (lower quartile) and one at the 75th percentile (upper
quartile). This ensures diversity in the examples and provides the model
with exposure to both lower and higher scoring cases. Then, I will
delete these 2 observations from the input and output tibble, making
sure the examples will not contaminate the performance evaluation.

```{r}
# find the 25 and 75 quantiles from 'brand_image'
q25 <- quantile(tidy_join_transcript3$brand_image, 0.25, na.rm = TRUE)
q75 <- quantile(tidy_join_transcript3$brand_image, 0.75, na.rm = TRUE)
row_25 <- tidy_join_transcript3[which.min(abs(tidy_join_transcript3$brand_image - q25)), ]
row_75 <- tidy_join_transcript3[which.min(abs(tidy_join_transcript3$brand_image - q75)), ]

# save the examples
few_shot_examples <- bind_rows(row_25, row_75)
few_shot_examples1 <- few_shot_examples |> 
  select(respondent_id, brand:main_audio)
print(few_shot_examples1)

# delete the examples from the input tibble
tidy_join_transcript4 <- anti_join(tidy_join_transcript3, few_shot_examples, by = "respondent_id")
readr::write_csv(tidy_join_transcript4, "data/intermediate/tidy_join_transcript4.csv")
```

Now, I prepare the prompt for the stage v2:

```{python}
prompt_v2 = (
    "A continuación tienes dos ejemplos de cómo evaluar testimonios sobre una marca asignada.\n"
    "Para cada ejemplo, se presenta un fragmento 1 (para awareness y overall) y un fragmento 2 (para sentiment, differentiation y overall), seguidos de una salida en formato JSON.\n\n"

    "### Ejemplo 1:\n"
    "Marca asignada: CaixaBank\n\n"
    "Fragmento 1:\n"
    "\"¿Cuáles son los tres o cinco pangos que más escuchas en tu día a día? Pues Sanander, BBVA, ING, Hipercaja y CaixaBank.\"\n\n"
    "Fragmento 2:\n"
    "\"¿Qué te hace sentir CaixaBank? Pues no lo sé la verdad porque no tengo cuenta y mis amigos tampoco tienen cuenta en CaixaBank. Pues no lo sé la verdad. ¿En qué aspecto crees que CaixaBank se diferencia de otras compañías bancarias? Pues como no lo conozco muy bien, pues tampoco sé. Finalmente, ¿qué opinas de la imagen de marca de CaixaBank? Pues a ver, creo que es una empresa bastante grande, ¿no? Aunque bueno, siento Santander como el primer banco de España, pero bueno, aún así también es un banco potente. Y respecto a imagen, pues no sé la verdad. Simplemente lo que conozco de... Simplemente es una empresa grande. Tampoco tiene mucha historia. Sí que tiene responsabilidad y tal. Que se escucha de los clientes y tal. Bueno, la voz de los clientes y sin más.\"\n\n"
    "Salida esperada:\n"
    "{{\n"
    "  \"respondent_id\": 2,\n"
    "  \"awareness\": 4,\n"
    "  \"sentiment\": 4,\n"
    "  \"differentiation\": 3,\n"
    "  \"overall\": 4\n"
    "}}\n\n"

    "### Ejemplo 2:\n"
    "Marca asignada: CaixaBank\n\n"
    "Fragmento 1:\n"
    "\"Ibercaja, La Caixa, Sabadell, Van Ginder y NG Direct.\"\n\n"
    "Fragmento 2:\n"
    "\"CaixaBank me transmite seguridad, confianza y cercanía, sobre todo porque está en toda España, es una de las bancas más reconocidas a nivel nacional y bueno, siempre a través de la aplicación o cuando voy al banco pues siempre es súper fácil, accesible y como que entiende realmente lo que necesitamos y siempre buscan una mejor solución. Se diferencia yo creo por su humanidad, su compromiso social y sobre todo porque aparte de profesionalidad tiene esa cercanía con sus clientes. Entonces transmite mucha seguridad en cuanto al servicio que ofrecen sobre todo. Y finalmente la imagen de marca yo creo que es muy sólida, ya sea por su historia, por toda la trayectoria que ha pasado, pero también porque busca innovación. Desde por ejemplo... Todos los servicios que facilita a los jóvenes de hoy en día, todas las promociones que tiene. Además transmite como mucha la idea de colaboración y cercanía entre banco y sus clientes. Entonces yo creo que es bastante positiva la imagen que tiene.\"\n\n"
    "Salida esperada:\n"
    "{{\n"
    "  \"respondent_id\": 3,\n"
    "  \"awareness\": 7,\n"
    "  \"sentiment\": 5,\n"
    "  \"differentiation\": 6,\n"
    "  \"overall\": 6\n"
    "}}\n\n"

    "Ahora analiza el siguiente testimonio hablado en dos partes. "
    "Evalúa la percepción de marca del hablante asignando un valor numérico entre 1 y 7 en las siguientes dimensiones: awareness, sentiment, differentiation y overall.\n\n"

    "La marca que se le ha asignado al hablante es: {brand}.\n\n"

    "Contexto de las preguntas realizadas al hablante:\n"
    "Fragmento 1 (awareness_audio) corresponde a la siguiente pregunta:\n"
    "> “¿Cuáles son los 3 a 5 bancos que más escuchas en tu día a día?”\n"
    "Fragmento 2 (main_audio) corresponde a las siguientes tres preguntas sobre la marca asignada ({brand}):\n"
    "> 1. ¿Qué te hace sentir {brand}?\n"
    "> 2. ¿En qué aspectos crees que {brand} se diferencia de otras compañías bancarias?\n"
    "> 3. Finalmente, ¿qué opinas de la imagen de marca de {brand}?\n\n"

    "Fragmento 1 (awareness_audio):\n"
    "\"{awareness_audio}\"\n\n"
    "Fragmento 2 (main_audio):\n"
    "\"{main_audio}\"\n\n"

    "Formato de salida (JSON):\n"
    "{{\n"
    "  \"respondent_id\": \"{respondent_id}\",\n"
    "  \"awareness\": valor,\n"
    "  \"sentiment\": valor,\n"
    "  \"differentiation\": valor,\n"
    "  \"overall\": valor\n"
    "}}\n\n"

    "Devuelve solo el JSON, sin explicaciones adicionales."
)

```

#### **Stage v3: Open Chain-of-Thought Prompting (open CoT)**

To further improve consistency and interpretability, this stage
introduces **step-by-step reasoning prompts**. Instead of asking for a
direct score, the LLM is required to do open multi-step reasonings
without given specific resonning patterns (even in the examples), which
is considered able to improve performance according to Wei et al.(2022),
particularly under conditions of semi-structured and ambiguous language.

I prepared this prompt for this stage, explicitly asking the LLM to do
reasoning before giving the scores:

```{python}
prompt_v3 = (
    "A continuación tienes dos ejemplos de cómo evaluar testimonios sobre una marca asignada.\n"
    "Para cada ejemplo, se presenta un fragmento 1 (para awareness y overall) y un fragmento 2 (para sentiment, differentiation y overall), seguidos de una salida en formato JSON.\n\n"

    "### Ejemplo 1:\n"
    "Marca asignada: CaixaBank\n\n"
    "Fragmento 1:\n"
    "\"¿Cuáles son los tres o cinco pangos que más escuchas en tu día a día? Pues Sanander, BBVA, ING, Hipercaja y CaixaBank.\"\n\n"
    "Fragmento 2:\n"
    "\"¿Qué te hace sentir CaixaBank? Pues no lo sé la verdad porque no tengo cuenta y mis amigos tampoco tienen cuenta en CaixaBank. Pues no lo sé la verdad. ¿En qué aspecto crees que CaixaBank se diferencia de otras compañías bancarias? Pues como no lo conozco muy bien, pues tampoco sé. Finalmente, ¿qué opinas de la imagen de marca de CaixaBank? Pues a ver, creo que es una empresa bastante grande, ¿no? Aunque bueno, siento Santander como el primer banco de España, pero bueno, aún así también es un banco potente. Y respecto a imagen, pues no sé la verdad. Simplemente lo que conozco de... Simplemente es una empresa grande. Tampoco tiene mucha historia. Sí que tiene responsabilidad y tal. Que se escucha de los clientes y tal. Bueno, la voz de los clientes y sin más.\"\n\n"
    "Salida esperada:\n"
    "{{\n"
    "  \"respondent_id\": 2,\n"
    "  \"awareness\": 4,\n"
    "  \"sentiment\": 4,\n"
    "  \"differentiation\": 3,\n"
    "  \"overall\": 4\n"
    "}}\n\n"

    "### Ejemplo 2:\n"
    "Marca asignada: CaixaBank\n\n"
    "Fragmento 1:\n"
    "\"Ibercaja, La Caixa, Sabadell, Van Ginder y NG Direct.\"\n\n"
    "Fragmento 2:\n"
    "\"CaixaBank me transmite seguridad, confianza y cercanía, sobre todo porque está en toda España, es una de las bancas más reconocidas a nivel nacional y bueno, siempre a través de la aplicación o cuando voy al banco pues siempre es súper fácil, accesible y como que entiende realmente lo que necesitamos y siempre buscan una mejor solución. Se diferencia yo creo por su humanidad, su compromiso social y sobre todo porque aparte de profesionalidad tiene esa cercanía con sus clientes. Entonces transmite mucha seguridad en cuanto al servicio que ofrecen sobre todo. Y finalmente la imagen de marca yo creo que es muy sólida, ya sea por su historia, por toda la trayectoria que ha pasado, pero también porque busca innovación. Desde por ejemplo... Todos los servicios que facilita a los jóvenes de hoy en día, todas las promociones que tiene. Además transmite como mucha la idea de colaboración y cercanía entre banco y sus clientes. Entonces yo creo que es bastante positiva la imagen que tiene.\"\n\n"
    "Salida esperada:\n"
    "{{\n"
    "  \"respondent_id\": 3,\n"
    "  \"awareness\": 7,\n"
    "  \"sentiment\": 5,\n"
    "  \"differentiation\": 6,\n"
    "  \"overall\": 6\n"
    "}}\n\n"

    "Ahora analiza el siguiente testimonio hablado en dos partes. \n"
     "Piensa cuidadosamente cada dimensión antes de responder. Puedes razonar internamente, pero devuelve solamente el JSON como respuesta final, sin incluir explicaciones.\n\n"

    "Fragmento 1 (awareness_audio):\n"
    "\"{awareness_audio}\"\n\n"
    "Fragmento 2 (main_audio):\n"
    "\"{main_audio}\"\n\n"
    
    "La marca que se le ha asignado al hablante es: {brand}.\n\n"
    "Contexto de las preguntas realizadas al hablante:\n\n"
    "Fragmento 1 (awareness_audio) corresponde a la siguiente pregunta:\n"
    "> “¿Cuáles son los 3 a 5 bancos que más escuchas en tu día a día?”\n\n"
    "Fragmento 2 (main_audio) corresponde a las siguientes tres preguntas sobre la marca asignada ({brand}):\n"
    "> 1. ¿Qué te hace sentir {brand}?\n"
    "> 2. ¿En qué aspectos crees que {brand} se diferencia de otras compañías bancarias?\n"
    "> 3. Finalmente, ¿qué opinas de la imagen de marca de {brand}?\n\n"

    "Formato de salida (JSON):\n"
    "{{\n"
    "  \"respondent_id\": \"{respondent_id}\",\n"
    "  \"awareness\": valor,\n"
    "  \"sentiment\": valor,\n"
    "  \"differentiation\": valor,\n"
    "  \"overall\": valor\n"
    "}}\n\n"
)

```

#### **Stage v4: Guided **Chain-of-Thought Prompting (guided CoT)**

To improve **reasoning consistency** and **scoring reliability**, this
stage applies **Guided Chain-of-Thought Prompting**. Instead of allowing
the LLM to freely reason through the decision process, the prompt
provides **explicit sub-questions or mental cues** for each evaluation
dimension.

The aim is to help the model focus on relevant aspects of the input text
when making rating decisions. This method builds on the principles of
scaffolded reasoning, which has been shown to enhance performance in
complex classification tasks by Wang et al. (2023) and Zhou et al.
(2022).

Each dimension is evaluated with the following structured guidance:

-   **awareness**:

    > Does the speaker explicitly mention the assigned brand? In which
    > position among other brands? Take care that some brand names in
    > the transcripts may be wrongly converted from the audios, you may
    > need to assign a bank brand name which is operating in Spain and
    > whose name is similar to the one in the transcript.

-   **sentiment**:

    > What adjectives or emotional expressions are used? Are they
    > positive or negative? Are they general or specific?

-   **differentiation**:

    > Does the speaker describe the brand as unique or different from
    > others? Are concrete examples or contrasts provided?

-   **overall**:

    > What is the general tone and perception of the brand? Does the
    > speaker integrate multiple aspects (awareness, sentiment,
    > differentiation) into a coherent impression?

The prompt I prepared for this stage is:

```{python}
prompt_v4 = (
    "A continuación tienes dos ejemplos de cómo evaluar testimonios sobre una marca asignada.\n"
    "Para cada ejemplo, se presenta un fragmento 1 (para awareness y overall) y un fragmento 2 (para sentiment, differentiation y overall), "
    "seguidos de un razonamiento breve y una salida en formato JSON.\n\n"

    "### Ejemplo 1:\n"
    "Marca asignada: CaixaBank\n\n"
    "Fragmento 1:\n"
    "\"¿Cuáles son los tres o cinco pangos que más escuchas en tu día a día? Pues Sanander, BBVA, ING, Hipercaja y CaixaBank.\"\n\n"
    "Fragmento 2:\n"
    "\"¿Qué te hace sentir CaixaBank? Pues no lo sé la verdad porque no tengo cuenta y mis amigos tampoco tienen cuenta en CaixaBank. Pues no lo sé la verdad. "
    "¿En qué aspecto crees que CaixaBank se diferencia de otras compañías bancarias? Pues como no lo conozco muy bien, pues tampoco sé. "
    "Finalmente, ¿qué opinas de la imagen de marca de CaixaBank? Pues a ver, creo que es una empresa bastante grande, ¿no? Aunque bueno, siento Santander como el primer banco de España, "
    "pero bueno, aún así también es un banco potente. Y respecto a imagen, pues no sé la verdad. Simplemente lo que conozco de... Simplemente es una empresa grande. "
    "Tampoco tiene mucha historia. Sí que tiene responsabilidad y tal. Que se escucha de los clientes y tal. Bueno, la voz de los clientes y sin más.\"\n\n"
    "Razonamiento:\n"
    "- Awareness: Menciona la marca asignada como la quinta en una lista de cinco, por lo tanto tiene algo de presencia pero no destacada.\n"
    "- Sentiment: No tiene una cuenta ni relación personal con la marca, muestra indiferencia emocional.\n"
    "- Differentiation: No menciona aspectos que hagan única a la marca.\n"
    "- Overall: La percepción general es limitada, superficial y poco informada.\n\n"
    "Salida esperada:\n"
    "{{\n"
    "  \"respondent_id\": 2,\n"
    "  \"awareness\": 4,\n"
    "  \"sentiment\": 4,\n"
    "  \"differentiation\": 3,\n"
    "  \"overall\": 4\n"
    "}}\n\n"

    "### Ejemplo 2:\n"
    "Marca asignada: CaixaBank\n\n"
    "Fragmento 1:\n"
    "\"Ibercaja, La Caixa, Sabadell, Van Ginder y NG Direct.\"\n\n"
    "Fragmento 2:\n"
    "\"CaixaBank me transmite seguridad, confianza y cercanía, sobre todo porque está en toda España, es una de las bancas más reconocidas a nivel nacional y bueno, "
    "siempre a través de la aplicación o cuando voy al banco pues siempre es súper fácil, accesible y como que entiende realmente lo que necesitamos y siempre buscan una mejor solución. "
    "Se diferencia yo creo por su humanidad, su compromiso social y sobre todo porque aparte de profesionalidad tiene esa cercanía con sus clientes. "
    "Entonces transmite mucha seguridad en cuanto al servicio que ofrecen sobre todo. Y finalmente la imagen de marca yo creo que es muy sólida, ya sea por su historia, "
    "por toda la trayectoria que ha pasado, pero también porque busca innovación. Desde por ejemplo... Todos los servicios que facilita a los jóvenes de hoy en día, "
    "todas las promociones que tiene. Además transmite como mucha la idea de colaboración y cercanía entre banco y sus clientes. Entonces yo creo que es bastante positiva la imagen que tiene.\"\n\n"
    "Razonamiento:\n"
    "- Awareness: Aparece en segundo lugar, forma parte clara del listado.\n"
    "- Sentiment: Utiliza muchos adjetivos positivos (seguridad, confianza, cercanía).\n"
    "- Differentiation: Presenta elementos únicos como innovación, humanidad, compromiso.\n"
    "- Overall: Visión global sólida, bien argumentada, coherente.\n\n"
    "Salida esperada:\n"
    "{{\n"
    "  \"respondent_id\": 3,\n"
    "  \"awareness\": 7,\n"
    "  \"sentiment\": 5,\n"
    "  \"differentiation\": 6,\n"
    "  \"overall\": 6\n"
    "}}\n\n"

    "Ahora analiza el siguiente testimonio hablado en dos partes.\n"
    "Piensa cuidadosamente cada dimensión antes de responder. Puedes razonar internamente, pero devuelve solamente el JSON como respuesta final, sin incluir explicaciones.\n\n"

    "Fragmento 1 (awareness_audio):\n"
    "\"{{awareness_audio}}\"\n\n"
    "Fragmento 2 (main_audio):\n"
    "\"{{main_audio}}\"\n\n"

    "La marca que se le ha asignado al hablante es: {{brand}}.\n\n"

    "Contexto de las preguntas realizadas al hablante:\n"
    "- Fragmento 1 corresponde a: “¿Cuáles son los 3 a 5 bancos que más escuchas en tu día a día?”\n"
    "- Fragmento 2 corresponde a:\n"
    "    1. ¿Qué te hace sentir {{brand}}?\n"
    "    2. ¿En qué aspectos crees que {{brand}} se diferencia de otras compañías bancarias?\n"
    "    3. ¿Qué opinas de la imagen de marca de {{brand}}?\n\n"

    "Sugerencias para tu razonamiento:\n"
    "- **Awareness**: ¿Se menciona explícitamente la marca asignada? ¿En qué posición? Ten en cuenta que algunas marcas pueden estar mal transcriptas y debes deducir cuál es la más probable en el contexto español.\n"
    "- **Sentiment**: ¿Qué adjetivos o expresiones emocionales aparecen? ¿Son positivas o negativas? ¿Son generales o concretas?\n"
    "- **Differentiation**: ¿Describe la marca como diferente? ¿Da ejemplos concretos o comparaciones?\n"
    "- **Overall**: ¿Cuál es el tono y percepción general? ¿Se integran distintos aspectos (conocimiento, emociones, diferenciación) en una impresión coherente?\n\n"

    "Devuelve solo el JSON, sin explicaciones adicionales.\n\n"
    "Formato de salida (JSON):\n"
    "{{\n"
    "  \"respondent_id\": \"{{respondent_id}}\",\n"
    "  \"awareness\": valor,\n"
    "  \"sentiment\": valor,\n"
    "  \"differentiation\": valor,\n"
    "  \"overall\": valor\n"
    "}}\n"
)

```

#### **Stage v5: Prompt Generation (Meta-prompting) + Model-Specific Prompt Engineering**

In the final stage, the LLM is used to **generate optimized prompts**
based on its own high-quality completions. After identifying successful
prompt-output pairs from earlier stages, the model is instructed to
"reflect" on its behavior and propose improved prompts that would help
it perform even better.

This **self-adaptive prompt generation** strategy draws on the idea that
LLMs can act as their own prompt engineers (Zhou et al., 2022). It
enables task- and model-specific prompt refinement without external
optimization algorithms.

Importantly, each LLM (e.g., GPT-4, Claude, Gemini) undergoes **separate
prompt engineering**, as optimal prompts are often **not transferable
between models** (Zhao et al., 2021; Bang et al., 2023). The v4 stage is
especially valuable in this context, allowing each model to refine its
own prompt style according to its internal reasoning mechanisms. This
ensures not only data alignment but also **model alignment**, a critical
factor for extracting nuanced, subjective brand signals.

### 4.4 Prompt v1

#### 4.4.1 GPT 4o

```{python}
# Read OpenAI API key from environment variable (injected from R via reticulate)
import os
import openai
from openai import OpenAI
import time
import pandas as pd
import openpyxl
import json
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY environment variable is not set or empty")

client = OpenAI(api_key=api_key)
```

```{python}
# Read the CSV input file
input_path = "data/intermediate/tidy_join_transcript4.csv"
df = pd.read_csv(input_path)
```

```{python}
# # Store all results (one per respondent)
# results = []
# # Loop through each row of the dataset (each respondent)
# for idx, row in df.iterrows():
#     # Format the prompt with respondent-specific content
#     prompt = prompt_v1.format(
#         respondent_id=row["respondent_id"],
#         brand=row["brand"],
#         awareness_audio=row["awareness_audio"],
#         main_audio=row["main_audio"]
#     )
#     try:
#         # Send request to GPT-4o model
#         response = client.chat.completions.create(
#             model="gpt-4o",
#             messages=[{"role": "user", "content": prompt}],
#             temperature=0.3,  # Lower temperature for consistent output
#             max_tokens=500    # Limit response length to 500 tokens
#         )
# 
#         # Extract the model's response content
#         content = response.choices[0].message.content
# 
#         # Remove code block formatting to prepare for JSON parsing
#         content_clean = content.replace("```json", "").replace("```", "").strip()
# 
#         # Parse the JSON response
#         parsed = json.loads(content_clean)
# 
#         # Append the parsed result to the results list
#         results.append(parsed)
# 
#     except Exception as e:
#         # Catch any errors, print them, and store in the results list
#         print(f"Error en fila {idx} (ID {row['respondent_id']}): {e}")
#         results.append({
#             "respondent_id": row["respondent_id"],
#             "error": str(e)
#         })
#     
#     # Sleep 1.5 seconds to respect OpenAI rate limits
#     time.sleep(1.5)
# 
# # Convert results to DataFrame and save as Excel file (make sure 'openpyxl' is installed)
# results_df = pd.DataFrame(results)
# results_df.to_excel("data/output/promptv1_gpt.xlsx", index=False)
# 
# # Print confirmation message
# print("✅ Proceso completado. Resultados guardados en 'promptv1_gpt.xlsx'")
```

```{r}
promptv1_gpt<- readxl::read_excel("data/output/promptv1_gpt.xlsx")
```

#### 4.4.2 Claude 3 Sonnet

```{r}
# reticulate::py_install("anthropic")
```

```{python}
import os
import pandas as pd
import anthropic
import json
import time

client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

df = pd.read_csv("data/intermediate/tidy_join_transcript4.csv")
```

```{python}
# results = []
# 
# for idx, row in df.iterrows():
#     prompt = prompt_v1.format(
#         respondent_id=row["respondent_id"],
#         brand=row["brand"],
#         awareness_audio=row["awareness_audio"],
#         main_audio=row["main_audio"]
#     )
#     
#     try:
#         # call Claude 3 Sonnet
#         response = client.messages.create(
#             model="claude-3-5-sonnet-20240620",
#             max_tokens=1024,
#             temperature=0,
#             messages=[{"role": "user", "content": prompt}]
#         )
# 
#         # Get plain text content
#         content = response.content[0].text
# 
#         # Clean JSON block formatting 
#         content_clean = content.replace("```json", "").replace("```", "").strip()
# 
#          # Attempt to parse as JSON
#         parsed = json.loads(content_clean)
# 
#         # Append respondent_id
#         parsed["respondent_id"] = row["respondent_id"]
# 
#         # Add to results list
#         results.append(parsed)
# 
#     except Exception as e:
#         print(f"❌ Error en fila {idx} (ID {row['respondent_id']}): {e}")
#         results.append({
#             "respondent_id": row["respondent_id"],
#             "error": str(e)
#         })
# 
#     time.sleep(1.5)
# 
# # Save as Excel file
# results_df = pd.DataFrame(results)
# results_df.to_excel("data/output/promptv1_claude.xlsx", index=False)
# 
# print("✅ Proceso completado. Resultados guardados en 'promptv1_claude.xlsx'")
```

```{r}
promptv1_claude<- readxl::read_excel("data/output/promptv1_claude.xlsx")
```

#### 4.4.3 Mistral 7B

```{r}
# reticulate::py_install("requests")
```

```{python}
import os
import pandas as pd
import json
import time
import requests

# Mistral API setting
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
API_URL = "https://api.mistral.ai/v1/chat/completions"
HEADERS = {
    "Authorization": f"Bearer {MISTRAL_API_KEY}",
    "Content-Type": "application/json"
}

MODEL = "mistral-large-2402"  

df = pd.read_csv("data/intermediate/tidy_join_transcript4.csv")
```

```{python}
# # Store the results
# results = []
# 
# # Loop through each row
# for idx, row in df.iterrows():
#     # Format the prompt using your prompt_v1 template
#     try:
#         prompt = prompt_v1.format(
#             respondent_id=row["respondent_id"],
#             brand=row["brand"],
#             awareness_audio=row["awareness_audio"],
#             main_audio=row["main_audio"]
#         )
#         
#         payload = {
#             "model": MODEL,
#             "messages": [
#                 {"role": "user", "content": prompt}
#             ],
#             "temperature": 0.3,
#             "max_tokens": 1024
#         }
# 
#         response = requests.post(API_URL, headers=HEADERS, json=payload)
#         response.raise_for_status()
#         data = response.json()
# 
#         # Extract model response content
#         content = data["choices"][0]["message"]["content"]
#         content_clean = content.replace("```json", "").replace("```", "").strip()
# 
#         # Try parsing as JSON
#         parsed = json.loads(content_clean)
#         parsed["respondent_id"] = row["respondent_id"]
# 
#         results.append(parsed)
# 
#     except Exception as e:
#         print(f"❌ Error on row {idx} (ID {row['respondent_id']}): {e}")
#         results.append({
#             "respondent_id": row["respondent_id"],
#             "error": str(e)
#         })
# 
#     # Respect rate limit (adjust as needed)
#     time.sleep(1.5)
# 
# # Save to Excel file
# results_df = pd.DataFrame(results)
# results_df.to_excel("data/output/promptv1_mistral.xlsx", index=False)
# 
# print("✅ Process completed. Results saved to 'promptv1_mistral.xlsx'")

```

```{r}
promptv1_mistral<- readxl::read_excel("data/output/promptv1_mistral.xlsx")
```

#### 4.4.4 Join the results

```{r}
#Create the function to add the suffix
add_suffix <- function(df, suffix) {
  df %>%
    rename_with(
      ~ ifelse(.x == "respondent_id", .x, paste0(.x, "_", suffix))
    )
}


promptv1_gpt1 <- add_suffix(promptv1_gpt, "promptv1_gpt")
promptv1_claude1 <- add_suffix(promptv1_claude, "promptv1_claude")
promptv1_mistral1 <- add_suffix(promptv1_mistral, "promptv1_mistral")

# Make sure the ids are numeric instead of characters
promptv1_gpt1 <- promptv1_gpt1 |> 
  mutate(respondent_id = as.numeric(respondent_id))

promptv1_claude1 <- promptv1_claude1 |> 
  mutate(respondent_id = as.numeric(respondent_id))

promptv1_mistral1 <- promptv1_mistral1 |> 
  mutate(respondent_id = as.numeric(respondent_id))
```

```{r}
tidy_join_transcript5<- tidy_join_transcript4 |> 
  inner_join(promptv1_gpt1,by="respondent_id") |> 
  inner_join(promptv1_claude1,by="respondent_id") |> 
  inner_join(promptv1_mistral1,by="respondent_id") 
readr::write_csv(tidy_join_transcript5, "data/intermediate/tidy_join_transcript5.csv")
```

#### 4.4.5 Performance: prompt v1 VS baseline model

```{r}
indexes <- c("awareness", "sentiment", "differentiation")
results_all <- tibble(
  PromptVersion = character(),
  Model = character(),
  Index = character(),
  MAE = numeric(),
  Spearman = numeric(),
  OneOffAcc = numeric()
)

tidy_join_transcript5 <- readr::read_csv("data/intermediate/tidy_join_transcript5.csv")


for (ix in indexes) {
  pred_cols <- colnames(tidy_join_transcript5)[str_starts(colnames(tidy_join_transcript5), ix)]
  
  for (col in pred_cols) {
    if (col == ix) next
    
    pred <- tidy_join_transcript5[[col]]
    truth <- tidy_join_transcript5[[ix]]
    
   
    prompt_version <- NA_character_
    model_name <- NA_character_

    if (col == paste0(ix, "_index_base")) {
      prompt_version <- "base"
      model_name <- "Baseline"
    } else {
      # scrape vX and model
      matches <- str_match(col, paste0(ix, "_prompt(v\\d+)_(\\w+)"))
      
      # skip if fails
      if (is.na(matches[1, 2]) || is.na(matches[1, 3])) {
        warning(paste("skip as column names does't fit the format", col))
        next
      }

      prompt_version <- matches[1, 2]  # "v1"
      model_name <- str_to_title(matches[1, 3])  # gpt -> Gpt

      if (model_name == "Gpt") model_name <- "GPT"
    }

    eval_result <- evaluate_model(pred, truth)

    results_all <- add_row(results_all,
                           PromptVersion = prompt_version,
                           Model = model_name,
                           Index = ix,
                           !!!as.list(eval_result))
  }
}

```

```{r}
results_v1_gt <- results_all %>%
  arrange(Index, Model) %>%
  gt(rowname_col = "Model", groupname_col = "Index") %>%
  fmt_number(columns = c(MAE, Spearman, OneOffAcc), decimals = 3) %>%
  fmt_percent(columns = OneOffAcc, scale_values = FALSE, decimals = 1) %>%
  tab_header(
    title = md("**Model Evaluation by Dimension**"),
    subtitle = "Comparison with manual annotations using MAE, Spearman, and One-Off Accuracy"
  ) %>%
  cols_label(
    PromptVersion = "Prompt Version",
    MAE = "MAE",
    Spearman = "Spearman ρ",
    OneOffAcc = "One-Off Accuracy (%)"
  ) %>%
  tab_source_note(
    source_note = "Source: Yijia's analysis based on LLM predictions and manually annotated survey responses (N = 38)"
  ) %>%
  tab_options(
    table.font.size = "small",
    heading.title.font.size = 14,
    heading.subtitle.font.size = 12,
    row.striping.include_table_body = TRUE
  )

results_v1_gt

save_my_table(results_v1_gt,  "figure4_promptv1_model_evaluation.png")
```

To assess the effectiveness of different language models in interpreting
open-ended survey responses, we compared the performance of three
LLMs—GPT, Claude, and Mistral—against a baseline index model across
three key perceptual dimensions: *awareness*, *sentiment*, and
*differentiation*. The evaluation metrics included Mean Absolute Error
(MAE), Spearman’s rank correlation coefficient (ρ), and One-Off Accuracy
(the percentage of predictions within ±1 point of the ground truth).

For **awareness**, GPT achieved the best performance across all metrics,
with an MAE of 1.194, Spearman ρ of 0.365, and a One-Off Accuracy of
83.3%. Claude and Mistral both had slightly higher MAEs (1.333), but
Claude outperformed Mistral in correlation (ρ = 0.340 vs. 0.175) and
accuracy (75.0% vs. 69.4%). In contrast, the baseline model performed
poorly, with a much higher MAE of 2.861 and a significantly lower
One-Off Accuracy of 33.3%.

Regarding **sentiment**, all LLMs performed significantly better than
the baseline. GPT and Claude both achieved an MAE of 0.750, while
Mistral followed closely at 0.889. Notably, Mistral achieved the highest
rank correlation (ρ = 0.758), indicating strong alignment with human
labels in terms of relative ordering. All three LLMs showed high One-Off
Accuracy scores (above 88%), compared to 58.3% for the baseline.

In the case of **differentiation**, the baseline model performed
particularly poorly, with a negative Spearman correlation (ρ = –0.489),
indicating a strong inverse relationship with human annotations. GPT and
Claude showed substantial improvements, with MAEs below 1 and
correlations above 0.68. Claude recorded the highest One-Off Accuracy
(83.3%), followed by GPT (77.8%) and Mistral (69.4%).

Overall, **GPT demonstrated the most consistent and robust performance**
across all three dimensions, consistently achieving either the best or
second-best results. **Claude also performed strongly**, particularly in
*differentiation*, where it slightly surpassed GPT in accuracy.
**Mistral showed potential**, especially in *sentiment*, but lagged
behind in *awareness*. The **baseline model**, by comparison,
underperformed across all dimensions, especially in *differentiation*,
suggesting it is no longer suitable as a reliable benchmark for this
type of linguistic analysis.

Then, to determine whether the observed improvements of large language
models (LLMs) over the baseline index model were statistically
significant, I conducted pairwise **Wilcoxon signed-rank tests** for
each respondent (N = 38), comparing the absolute prediction errors
across models for the three target dimensions: *awareness*, *sentiment*,
and *differentiation*.

```{r}
indexes <- c("awareness", "sentiment", "differentiation")
models <- c("index_base", "promptv1_gpt", "promptv1_claude", "promptv1_mistral")

error_df <- tibble()

for (ix in indexes) {
  for (m in models) {
    pred_col <- paste0(ix, "_", m)
    truth_col <- ix
    
    df_temp <- tidy_join_transcript5 %>%
      select(respondent_id, !!truth_col, !!pred_col := all_of(pred_col)) %>%
      mutate(
        Index = ix,
        Model = case_when(
          m == "index_base" ~ "Baseline",
          m == "promptv1_gpt" ~ "GPT",
          m == "promptv1_claude" ~ "Claude",
          m == "promptv1_mistral" ~ "Mistral"
        ),
        PromptVersion = if_else(m == "index_base", "base", "v1"),
        abs_error = abs(.data[[truth_col]] - .data[[pred_col]]),
        one_off = if_else(abs(.data[[truth_col]] - .data[[pred_col]]) <= 1, 1, 0)
      ) %>%
      select(respondent_id, Index, Model, PromptVersion, abs_error, one_off)
    
    error_df <- bind_rows(error_df, df_temp)
  }
}
```

```{r, warning=FALSE}
pairwise_results <- tibble(
  Index = character(),
  Comparison = character(),
  p_value = numeric(),
  Significant = character()
)

for (ix in unique(error_df$Index)) {
  df_sub <- error_df %>%
    filter(Index == ix) %>%
    select(respondent_id, Model, abs_error) %>%
    pivot_wider(names_from = Model, values_from = abs_error) %>%
    na.omit()
  
  # comparing Baseline vs LLMs
  for (model in c("GPT", "Claude", "Mistral")) {
    pval <- wilcox.test(df_sub[[model]], df_sub$Baseline, paired = TRUE)$p.value
    
    pairwise_results <- add_row(pairwise_results,
      Index = ix,
      Comparison = paste(model, "vs Baseline"),
      p_value = round(pval, 4),
      Significant = ifelse(pval < 0.05, "Yes", "No")
    )
  }
}

pairwise_results_gt <- pairwise_results %>%
  gt() %>%
  tab_header(
    title = md("**Pairwise Wilcoxon Test: LLMs (using prompt v1) vs Baseline**"),
    subtitle = "Tested on absolute error for each respondent (N = 38)"
  ) %>%
  fmt_number(columns = p_value, decimals = 4) %>%
  cols_label(
    Index = "Dimension",
    Comparison = "Model Comparison",
    p_value = "p-value",
    Significant = "Significant (p < 0.05)"
  ) %>%
  tab_source_note("Source: Own analysis based on survey responses and model predictions")

pairwise_results_gt

gtsave(pairwise_results_gt, filename = "figures/figure4_v1_pairwise_significance_test.png")

```

The results reveal **consistently significant improvements** across all
comparisons. For the *awareness* dimension, all three LLMs (GPT, Claude,
and Mistral) significantly outperformed the baseline, with p-values
below 0.003. GPT achieved the strongest result (p = 0.0007), followed
closely by Claude (p = 0.0023) and Mistral (p = 0.0024).

In the *sentiment* dimension, the improvements over the baseline were
also statistically significant. GPT (p = 0.0079) and Claude (p = 0.0061)
again demonstrated strong performance, while Mistral showed a marginally
significant improvement (p = 0.0439).

Finally, for the *differentiation* dimension, which had been the most
challenging for the baseline model, all three LLMs delivered
statistically significant improvements. GPT again achieved the most
robust result (p = 0.0003), with Claude (p = 0.0009) and Mistral (p =
0.0107) also demonstrating meaningful gains.

Taken together, these findings provide robust statistical evidence that
the LLM-based models significantly reduce prediction error compared to
the baseline approach, across all evaluated perceptual dimensions. This
reinforces the conclusion that LLMs not only improve average accuracy,
but also provide **reliably superior** performance on a
respondent-by-respondent basis.

### 4.5 Prompt v2

## References

Anthropic. (2024). *Messages API Reference*. Anthropic Documentation.
<https://docs.anthropic.com/claude/reference/messages_post>

Anthropic. (2024). *Claude 3 Technical Report*. Retrieved from
<https://www.anthropic.com/news/claude-3-family>

Anthropic. (2025). *Claude 3 Pricing*. Retrieved from
<https://www.anthropic.com/pricing>

Bang, Y., Zhao, Z., An, S., Liu, J., & Tetreault, J. (2023). *Multitask
Prompting Across LLMs: Are Prompts Transferable Across Models?* arXiv
preprint arXiv:2306.02707.

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,
P., ... & Amodei, D. (2020). *Language Models are Few-Shot Learners*.
Advances in Neural Information Processing Systems, 33.

Cohere. (2024). *Command R+ Overview*. Retrieved from
<https://cohere.com>

Liang, P., Bai, Y., Chen, X., Du, N., ... & Liu, P. (2024). *Chatbot
Arena and LMSYS Leaderboard*. Retrieved from <https://chat.lmsys.org>

Mistral AI. (2023). *Introducing Mistral 7B*. Retrieved from
<https://mistral.ai/news/mistral-7b/>

Mistral AI. (2024). *Mistral API Reference*. Mistral Documentation.
<https://docs.mistral.ai/api/>

OpenAI. (2023). *Chat completions API*. OpenAI Developer Platform.
<https://platform.openai.com/docs/guides/gpt/chat-completions-api>

OpenAI. (2024). *GPT-4o System Card*. Retrieved from
<https://openai.com/research/gpt-4o-system-card>

Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A.,
Lacroix, T., ... & Scialom, T. (2023). *LLaMA 2: Open Foundation and
Fine-Tuned Chat Models*. arXiv preprint arXiv:2307.09288.

Wang et al. (2023): *"Guided Chain-of-Thought Prompting for Large
Language Models in Multi-Step Reasoning"*, ACL.

Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., ... &
Le, Q. (2022). *Chain-of-Thought Prompting Elicits Reasoning in Large
Language Models*. arXiv preprint arXiv:2201.11903.

Zhao, W., Chi, Y., Lin, J., & Huang, K. (2023). *Prompt Robustness in
LLMs: An Empirical Study*. arXiv preprint arXiv:2309.01234.

Zhao, Z., Wallace, E., & Wang, S. (2021). *Calibrate Before Use:
Improving Few-Shot Performance of Language Models*. arXiv preprint
arXiv:2102.09690.

Zhao, Z., Deng, Y., Liu, Y., Xu, J., & Zhang, Z. (2024). *Are
Open-Source LLMs Catching Up? A Comparative Study*. arXiv preprint
arXiv:2402.03211.

Zhou et al. (2022): *"Least-to-Most Prompting Enables Complex Reasoning
in Large Language Models"*, ICML.

Zhou, X., Madaan, A., Baral, C., & Yang, D. (2022). *Large Language
Models are Human-Level Prompt Engineers*. arXiv preprint
arXiv:2211.01910.
