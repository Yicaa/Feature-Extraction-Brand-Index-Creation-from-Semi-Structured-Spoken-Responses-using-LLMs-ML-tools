---
title: "TFM - Classification of semi-structured answers in audio"
author: "Yijia Lin, supervised by Francisco Javier Nogales Martín"
date: "`r Sys.Date()`"
output: 
  rmdformats::html_clean:
    lightbox: false
    thumbnails: false
    toc: yes
    toc_float: true
    toc_depth: 3
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
reticulate::use_python("/Users/linyijia/whisper310_env/bin/python", required = TRUE)
```

## Prerequisites & Packages

To run the audio transcription and classification pipeline, the
following tools and libraries must be installed:

1.  **FFmpeg**

Install using:

```{bash}
#In Terminal (not Rmd), already done:
# brew install ffmpeg  
# if the computer is MacOS
```

FFmpeg is a command-line tool for processing audio and video files. It
is required because OpenAI Whisper depends on FFmpeg to load and convert
audio files (e.g., .mp3, .wav, .m4a) into a format suitable for
transcription. On the other hand, some other file format like .amr or
.mov could be not be read by OpenAI Whisper, and this tool can help with
format transformation. Installing it via Homebrew ensures it is
available system-wide on macOS.

<br>

2.  **The IDE of R studio and the interpreter of Python 3**

This notebook will be using both R and Python languages, so it's
important to make sure the R studio and Python 3 are installed.

These steps below are intended to ensure that Python 3.10 and the
required development libraries (e.g., zlib, llvm) are properly
installed. Creating a dedicated Python virtual environment
whisper310_env ensures that the packages used for Whisper will not
conflict with other Python environments on your system.

```{bash}
#In Terminal (not Rmd), already done:
# brew install zlib llvm
# python3.10 -m venv ~/whisper310_env
# source ~/whisper310_env/bin/activate
```

<br>

3.  **R Libraries**

Install using:

```{r, message=FALSE, warning=FALSE}
#install.packages("reticulate")
library(reticulate)
library(tidyverse)
library(readxl)
library(conflicted)
library(stringr)
#install.packages("googledrive")
library(googledrive)
library(patchwork)
library(ggrepel)
library(extrafont)
library(stringdist)
library(tidytext)
library(readr)
library(scales)
library(xml2)
library(spacyr)

#avoid conflicts in packages
conflicted::conflict_prefer("filter", "dplyr")
conflicted::conflict_prefer("select", "dplyr")
```

<br>

4.  **Python Libraries**

On macOS, especially Apple Silicon (M1/M2/M3), Whisper depends on
several low-level dynamic libraries (e.g., zlib, libc++, libunwind) for
numerical computations. These libraries are not installed or linked by
default for R + Python interoperability. If dynamic linking errors occur
(e.g., Library not loaded), you may need to manually copy these
libraries into R’s library search path like I did as shown below.

```{bash}
# No need to manually set environment variables (LDFLAGS, CPPFLAGS, DYLD_LIBRARY_PATH), unless you encounter dynamic library errors. In that case, please contact the author for troubleshooting.

# sudo cp /opt/homebrew/opt/llvm/lib/c++/libc++.1.dylib /Library/Frameworks/R.framework/Resources/lib/
# sudo cp /opt/homebrew/opt/zlib/lib/libz.1.dylib /Library/Frameworks/R.framework/Resources/lib/
# sudo cp /opt/homebrew/opt/llvm/lib/c++/libc++abi.1.dylib /Library/Frameworks/R.framework/Resources/lib/
# sudo cp /opt/homebrew/Cellar/llvm/20.1.6/lib/unwind/libunwind.1.dylib /Library/Frameworks/R.framework/Resources/lib/


```

```{r}
# No need to manually set environment variables (LDFLAGS, CPPFLAGS, DYLD_LIBRARY_PATH), unless you encounter dynamic library errors. In that case, please contact the author for troubleshooting.

# Sys.getenv("DYLD_LIBRARY_PATH")
# Sys.setenv(
#   LDFLAGS="-L/opt/homebrew/opt/zlib/lib -L/opt/homebrew/opt/llvm/lib",
#   CPPFLAGS="-I/opt/homebrew/opt/zlib/include -I/opt/homebrew/opt/llvm/include",
#   PATH="/opt/homebrew/opt/llvm/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin"
# )
```

Install using:

```{bash}
#In Terminal (not Rmd), already done:
# pip install --upgrade pip
# pip install openai-whisper pandas ffmpeg-python
```

These libraries serve the following purposes:

**openai-whisper**: Enables local speech-to-text transcription using
OpenAI's Whisper model.

**openai**: Provides access to OpenAI's API (e.g., GPT-4) to analyze and
classify open-ended responses.

**pandas**: A data analysis library used for reading Excel files,
applying classification functions row-by-row, and exporting results.

Now, we connect the r studio with the virtual python environment by
using `reticulate`:

```{r, message=FALSE, warning=FALSE, cache=TRUE}
reticulate::use_virtualenv("~/whisper310_env", required = TRUE)
py_config()
```

<br>

## Data Cleaning & Processing

### Read the data

```{r, message=FALSE, warning=FALSE}
raw <- read_excel("data/raw data/Survey data.xlsx")
```

### Tidy and clean the data

The first step will be cleaning the data! Given the limit of structure
of Google Forms, the raw spreadsheet is not in the cleanest way, so I
will firstly try to tidy it. For any bank assigned to the respondents,
the structure of the survey was the same. Therefore, I'll try to combine
all the information in the same columns instead of having dobbled
columns.

Before that, I'll specify for each row the assigned bank according to
the random assignation:

```{r}
raw1 <- raw |>
  rename(assignation = `Asignación de marca: Selecciona una opción aleatoria para que se te asigne una marca.`) |>
  mutate(bank = case_when(
    str_detect(assignation, "◻") ~ "BBVA",
    str_detect(assignation, "◇") ~ "BBVA",
    str_detect(assignation, "○") ~ "CaixaBank",
    str_detect(assignation, "⌔") ~ "CaixaBank",
    str_detect(assignation, "⭐") ~ "CaixaBank",
    TRUE ~ NA_character_
  )) |> 
  relocate(bank, .after = assignation)
```

Then, I will process separately the data for BBVA and CaixaBank before
joining them into the main table:

```{r}
raw1 <- raw1 |> 
  filter(if_any(
    .cols = matches("^La marca de banco que se te ha asignado es: (BBVA|CaixaBank)"),
    ~ !is.na(.)
  )) |> 
  mutate(respondent_id = row_number()) |> #Create respondent ID for later joint
  relocate(respondent_id, .after = "Marca temporal")
```

```{r}
bbva_long <- raw1 |> 
  filter(bank == "BBVA") |> 
  select(respondent_id, bank,
         awareness_audio = "Por favor, graba un audio de 10 segundos (y súbelo aquí) respondiendo a la siguiente pregunta:\n\n¿Cuáles son los 3 a 5 bancos que más escuchas en tu día a día?...9"    ,
         main_audio = matches("^La marca de banco que se te ha asignado es: BBVA"),
         awareness = matches("^¿Qué nivel de reconocimiento tiene BBVA"),
         sentiment = matches("^¿Cómo es la emoción que genera la marca BBVA"),
         differentiation = matches("^¿Qué opinas sobre la diferenciación de BBVA"),
         brand_image = matches("^¿Qué opinas de la imagen de marca de BBVA")
  ) |>
  mutate(brand = "BBVA") |>
  select(respondent_id, brand,  awareness, sentiment, differentiation,brand_image, awareness_audio, main_audio)


```

```{r}
caixa_long <- raw1 |> 
  filter(bank == "CaixaBank") |> 
  select(respondent_id, bank,
         awareness_audio = "Por favor, graba un audio de 10 segundos (y súbelo aquí) respondiendo a la siguiente pregunta:\n\n¿Cuáles son los 3 a 5 bancos que más escuchas en tu día a día?...15" ,
         main_audio = matches("^La marca de banco que se te ha asignado es: CaixaBank"),
         awareness = matches("^¿Qué nivel de reconocimiento tiene CaixaBank"),
         sentiment = matches("^¿Cómo es la emoción que genera la marca CaixaBank"),
         differentiation = matches("^¿Qué opinas sobre la diferenciación de CaixaBank"),
         brand_image = matches("^Finalmente, ¿qué opinas de la imagen de marca de CaixaBank?")
  ) |>
  mutate(brand = "CaixaBank") |>
  select(respondent_id, brand,  awareness, sentiment, differentiation,brand_image, awareness_audio, main_audio)

```

```{r}
tidy <- bind_rows(bbva_long, caixa_long)
tidy_survey_data <- raw1 |> 
  select(1:8) |> 
  inner_join(tidy, by = "respondent_id") 
```

### Download the audios in bulk

To download the audios in bulk, a Google authorization of my personal
account is required as the audios should be private and only accessible
to me.

```{r, cache=TRUE}
#drive_auth()
```

Then I prepare the list of links for downloading:

```{r, cache=TRUE}
awareness_audio<- unique(tidy_survey_data$awareness_audio)
main_audio<- unique(tidy_survey_data$main_audio)
```

Now, I download the audios and save them with ids in 2 different folders
according to the type of the audio:

```{r, message=FALSE, cache=TRUE}
# dir.create(file.path("data", "awareness_audio"), showWarnings = FALSE)
# dir.create(file.path("data", "main_audio"), showWarnings = FALSE)
# 
# for (i in seq_along(main_audio)) {
#   file_url <- main_audio[i]
#   try({
#     drive_file <- drive_get(as_id(file_url))
#     
#     # get the original file extension
#     original_name <- drive_file$name
#     ext <- tools::file_ext(original_name)
#     
#     # generate the final file path
#     output_path <- file.path("data", "main_audio", paste0("audio_", i, ".", ext))
#     
#     # download the audio
#     drive_download(drive_file, path = output_path, overwrite = TRUE)
#   }, silent = TRUE)
# }
# 
# for (i in seq_along(awareness_audio)) {
#   file_url <- awareness_audio[i]
#   try({
#     drive_file <- drive_get(as_id(file_url))
#     
#     # get the original file extension
#     original_name <- drive_file$name
#     ext <- tools::file_ext(original_name)
#     
#     # generate the final file path
#     output_path <- file.path("data", "awareness_audio", paste0("audio_", i, ".", ext))
#     
#     # download the audio
#     drive_download(drive_file, path = output_path, overwrite = TRUE)
#   }, silent = TRUE)
# }
```

### Convert audios into text locally using OpenAI Whisper

#### Make sure the audio formats are compatible with OpenAI Whisper

Firstly, I need to convert some special formats of audios into formats
which are compatibles with OpenAI Whisper.

```{r, cache=TRUE}
# Create a new folder for saving the converted audios
# dir.create(file.path("data", "processed_main_audio"), showWarnings = FALSE)
# dir.create(file.path("data", "processed_awareness_audio"), showWarnings = FALSE)
```

```{r, cache=TRUE, message=FALSE, warning=FALSE}
# Set folder with original audio files
input_folder <- "data/main_audio"  

# Set folder where processed files will be saved
output_folder <- "data/processed_main_audio"
dir.create(output_folder, showWarnings = FALSE)

# Get all file paths in the input folder
files <- list.files(input_folder, full.names = TRUE)


# Loop through each file
for (f in files) {
  # Get file extension (case-insensitive)
  ext <- tools::file_ext(f)
  
  # Construct output file path (use .wav if converting)
  base_name <- tools::file_path_sans_ext(basename(f))
  
  # If incompatible format (amr or mov), convert to .wav
  if (tolower(ext) %in% c("amr", "mov")) {
    output_file <- file.path(output_folder, paste0(base_name, ".wav"))
    system(sprintf('ffmpeg -y -i "%s" "%s"', f, output_file))
  } else {
    # If compatible format, just copy the file as is
    output_file <- file.path(output_folder, basename(f))
    file.copy(f, output_file, overwrite = TRUE)
  }
}
```

```{r, message=FALSE, warning=FALSE, cache=TRUE}
# # Repeat the same process
# input_folder <- "data/awareness_audio"  
# 
# # Set folder where processed files will be saved
# output_folder <- "data/processed_awareness_audio"
# dir.create(output_folder, showWarnings = FALSE)
# 
# # Get all file paths in the input folder
# files <- list.files(input_folder, full.names = TRUE)
# 
# 
# # Loop through each file
# for (f in files) {
#   # Get file extension (case-insensitive)
#   ext <- tools::file_ext(f)
#   
#   # Construct output file path (use .wav if converting)
#   base_name <- tools::file_path_sans_ext(basename(f))
#   
#   # If incompatible format (amr or mov), convert to .wav
#   if (tolower(ext) %in% c("amr", "mov")) {
#     output_file <- file.path(output_folder, paste0(base_name, ".wav"))
#     system(sprintf('ffmpeg -y -i "%s" "%s"', f, output_file))
#   } else {
#     # If compatible format, just copy the file as is
#     output_file <- file.path(output_folder, basename(f))
#     file.copy(f, output_file, overwrite = TRUE)
#   }
# }
```

#### Convert the audios

```{python, cache=TRUE}
# import ssl
# ssl._create_default_https_context = ssl._create_unverified_context #solve certification problems, as here we are doing python in r studio
# 
# import whisper
# model = whisper.load_model("large")
# 
# import os  
# import pandas as pd
# 
# # Set the folder path that contains the audio files
# AUDIO_FOLDER = "data/processed_main_audio"
# 
# 
# # Supported audio formats (incompatible ones will be skipped)
# AUDIO_EXTENSIONS = [".mp3", ".wav", ".m4a", ".mp4", ".webm", ".ogg", ".flac"]
# 
# # List to store results: filename and its transcript
# results = []
# 
# # Loop through all audio files in the folder
# for filename in os.listdir(AUDIO_FOLDER):
#     if any(filename.endswith(ext) for ext in AUDIO_EXTENSIONS):
#         audio_path = os.path.join(AUDIO_FOLDER, filename)
#         print(f"Transcribiendo: {filename}...")  
# 
#         # Transcribe audio to text with Whisper
#         result = model.transcribe(audio_path, language="Spanish")
#         transcript = result["text"]  # Get the transcribed text
# 
#         # Add result to list (filename + transcript)
#         results.append({
#             "file": filename,
#             "transcript": transcript
#         })
# 
# # Save all transcripts into a single CSV file for analysis or R
# df = pd.DataFrame(results)
# csv_path = os.path.join(AUDIO_FOLDER, "transcripts_main.csv")
# df.to_csv(csv_path, index=False, encoding="utf-8")
# 
# print("¡Transcripción completada y guardada en transcripts.csv!")

# Note:
# If you see the warning:
# "UserWarning: FP16 is not supported on CPU; using FP32 instead"
# This is expected. Whisper automatically switches from half precision (FP16) to single precision (FP32) on CPU-only devices. It does not affect the transcription results but may slightly impact performance.
```

```{python, cache=TRUE, message=FALSE, warning=FALSE}
# # Repeat the same processes for awareness audios
# AUDIO_FOLDER = "data/processed_awareness_audio"
# 
# # Supported audio formats (incompatible ones will be skipped)
# AUDIO_EXTENSIONS = [".mp3", ".wav", ".m4a", ".mp4", ".webm", ".ogg", ".flac"]
# 
# # List to store results: filename and its transcript
# results = []
# 
# # Loop through all audio files in the folder
# for filename in os.listdir(AUDIO_FOLDER):
#     if any(filename.endswith(ext) for ext in AUDIO_EXTENSIONS):
#         audio_path = os.path.join(AUDIO_FOLDER, filename)
#         print(f"Transcribiendo: {filename}...")  
# 
#         # Transcribe audio to text with Whisper
#         result = model.transcribe(audio_path, language="Spanish")
#         transcript = result["text"]  # Get the transcribed text
# 
#         # Add result to list (filename + transcript)
#         results.append({
#             "file": filename,
#             "transcript": transcript
#         })
# 
# # Save all transcripts into a single CSV file for analysis or R
# df = pd.DataFrame(results)
# csv_path = os.path.join(AUDIO_FOLDER, "transcripts_awareness.csv")
# df.to_csv(csv_path, index=False, encoding="utf-8")
# 
# print("¡Transcripción completada y guardada en transcripts.csv!")
```

After converting the audios into texts (saved in csv format), now we
read the csv files to access them in R studio:

```{r, cache=TRUE}
transcrip_main <- read.csv("data/processed_main_audio/transcripts_main.csv")
transcrip_awareness <- read.csv("data/processed_awareness_audio/transcripts_awareness.csv")
```

### Join and clean the transcripts

In case there are audios not conform to requirements, I'll do a further
cleaning: filtering out the transcripts which contains less than 20
words (as they're supposed to be 1 min long).

```{r, cache=TRUE}
transcript<- transcrip_main |> 
  left_join(transcrip_awareness, by = "file") |> 
  mutate(respondent_id= as.integer(sub("^audio_(\\d+)\\..*$", "\\1", file))) |> 
  rename(awareness_audio=transcript.y,
         main_audio=transcript.x) |> 
  mutate(
    clean_text = str_remove_all(main_audio, "[[:punct:]]"),  
    word_count = str_count(clean_text, "\\S+")                
  ) |> 
  filter(word_count >= 20) |> 
  select(respondent_id,awareness_audio,main_audio)
```

```{r, cache=TRUE}
tidy_join_transcript <- tidy_survey_data |> 
  select(-c("awareness_audio","main_audio")) |> 
  inner_join(transcript, by = "respondent_id") |> 
  rename(CCAA="¿En cuál de las siguientes Comunidades/Ciudades Autónomas vives?")
```

### Balance check

I'll first check if the random assignation gave a fair split of
observations for each of the banks:

```{r, message=FALSE, warning=FALSE}
#set the same theme for all plots later
loadfonts(device = "pdf") #macOs
my_theme <- theme_void() +
  theme(
    legend.position = "right",
    legend.title = element_text(size = 10, face = "bold", family = "Times New Roman"),
    legend.text = element_text(size = 9, family = "Times New Roman"),
    plot.title = element_text(
      size = 12,
      face = "bold",
      hjust = 0.5,
      family = "Times New Roman",
      margin = margin(b = 10)
    ),
    plot.caption = element_text(
      size = 8,
      family = "Times New Roman",
      hjust = 1,
      color = "gray40"
    ),
    plot.margin = margin(t = 20, r = 40, b = 20, l = 40),
    legend.box.margin = margin(l = 10),
    text = element_text(family = "Times New Roman")
  )

# create the function to save the plots in the same format
fig_dir <- "figures"
if (!dir.exists(fig_dir)) {
  dir.create(fig_dir)
}

save_my_plot <- function(p, filename) {
  ggsave(file.path(fig_dir, filename),
          plot = p, dpi = 600, width = 9, height = 5, units = "in")
}
```

```{r brand balance, cache=TRUE}
freq_bank <- tidy_join_transcript |> 
  count(brand) |> 
  mutate(
    percent = n / sum(n),
    label = scales::percent(percent, accuracy = 0.1)  
  )


p1 <- ggplot(freq_bank, aes(x = "", y = percent, fill = brand)) +
  geom_bar(stat = "identity", width = 1, color = "white", linewidth = 0.3) +
  coord_polar("y", start = 0) +
  ggrepel::geom_text_repel(
    aes(label = label),
    position = position_stack(vjust = 0.5),
    size = 5,
    color = "white",  
    point.padding = 0.2,
    min.segment.length = 0,  
    segment.color = "gray30",
    segment.size = 0.3,
    show.legend = FALSE
  ) +
  labs(
    title = "Distribution of Audios between BBVA and CaixaBank",
    fill = "Bank Brand",
    caption = "Source: Yijia's analysis"
  ) +
  scale_fill_manual(
    values = c("#1f77b4", "#ff7f0e"),  
    labels = function(x) tools::toTitleCase(tolower(x)) 
  ) +
  my_theme +
  theme(
    plot.caption = element_text(
      size = 9,
      hjust = 1,
      family = "Times New Roman",
      face = "italic",
      margin = margin(t = 10)
    )
  )
p1
```

```{r, cache=TRUE}
save_my_plot(p1, "figure1_bank_distribution.png")
```

According to the pie plot, I have balanced observations for both brands
(19 observations of each).

Now, I would like to check the distribution among the social-economic
variables, although the sample itself is not representative and then I
didn't expect a very balanced sample.

```{r socio-economic balance, cache=TRUE}
create_pie_chart <- function(data, variable, title) {

  freq_data <- data |> 
    count({{variable}}) |> 
    drop_na() |> 
    mutate(
      percent = n / sum(n),
      label_raw = paste0(scales::percent(percent, accuracy = 1), "\n", as.character({{variable}}))
    ) |> 
    arrange(desc(percent)) |> 
    mutate(label = ifelse(row_number() <= 3, label_raw, NA))  #only show labels for the top 3

  ggplot(freq_data, aes(x = "", y = percent, fill = {{variable}})) +
    geom_bar(stat = "identity", width = 1, color = "white") +
    coord_polar("y", start = 0) +
    geom_text(
      aes(label = label),
      position = position_stack(vjust = 0.5),
      size = 2.5,
      lineheight = 0.9,
      color = "black",
      show.legend = FALSE
    ) +
    labs(title = title) +
    theme_void() +
    theme(
      plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
      plot.margin = margin(5, 5, 5, 5),
      legend.position = "none"
    )
}



p1 <- create_pie_chart(tidy_join_transcript, Sexo, "Sexo")
p2 <- create_pie_chart(tidy_join_transcript, Edad, "Edad")
p3 <- create_pie_chart(tidy_join_transcript, Estudios, "Estudios")
p4 <- create_pie_chart(tidy_join_transcript, `Ingresos mensuales brutos del hogar`, "Ingresos del Hogar")
p5 <- create_pie_chart(tidy_join_transcript, `Clase Social`, "Clase Social")
p6 <- create_pie_chart(tidy_join_transcript, CCAA, "CCAA")


combined_plot <- (p1 + p2 + p3) / (p4 + p5 + p6) +
  plot_annotation(
    title = "Socialeconomic Variables Balance Check",
    caption = "Source: Yijia's analysis",
    theme = theme(
      plot.title = element_text(
        size = 14,
        face = "bold",
        hjust = 0.5,
        family = "Times New Roman",
        margin = margin(b = 10)
      ),
      plot.caption = element_text(        
        size = 9,
        hjust = 1,
        family = "Times New Roman",
        face = "italic",
        margin = margin(t = 10)
      )
    )
  )


combined_plot
```

```{r, cache=TRUE}
save_my_plot(combined_plot,"figure2_socialeconomic_variables_balance_check.png")
```

It's observed that only sexes are more or less balanced (58% male \~ 42%
female), but for other variables, it's not balanced: most respondents of
this survey have an age of 18-27 years old, they've been educated in
universities, the monthly house income is typically less than 2000€,
most belongs to medium social class, and live in Madrid.

However, the inblance in the sample doesn't matter in this research, as
I only try to investigate the accuracy of this methodology itself but
not intend to generalized the conclusions.

<br>

## Baseline model: Text Mining + BERT

Before introducing LLMs to generate the indexes, I'll firstly try to
create a baseline model for each of these indexes by text mining tools
and BERT, with the purpose of setting a minimum expectation of accuracy
for LLM models.

### Awareness index

```{r, cache=TRUE}
# create a brand dictionary（as Whisper usually makes mistakes especially while detecting brand names）
bank_aliases <- list(
  "BBVA" = c("BBVA", "BVA", "PPVA", "BVBA", "VVA"),
  "CaixaBank" = c("CaixaBank", "CaxaBank", "Cajabank", "Caixabán", "La Caixa", "la caixa", "Caixa", "La caixa", "La CaixaBank"),
  "Santander" = c("Santander", "Sandander", "Sanander", "San Ander", "Banco Santander", "Banc Santander"),
  "Sabadell" = c("Sabadell", "Banco Sabadell", "Sabadel", "Savadell"),
  "Ibercaja" = c("Ibercaja", "Hipercaja", "Vercaja"),
  "ING" = c("ING", "ING Direct", "ING Directa"),
  "Revolut" = c("Revolut"),
  "Openbank" = c("Open Bank", "Openbank"),
  "Unicaja" = c("Unicaja"),
  "Liberbank" = c("LiberBank", "Liberbank"),
  "Caja Rural" = c("Caja Rural"),
  "Imagine" = c("Imagine"),
  "Deutsche Bank" = c("Deutsche Bank"),
  "Evo" = c("Evo"),
  "Trade Republic" = c("Trade Republic"),
  "NG" = c("NG", "NG Direct", "NG."),
  "Cajamar" = c("Cajamar"),
  "Caixa Popular" = c("Caixa Popular"),
  "Banco Mars" = c("Banco Mars"),
  "Bankinter" = c("Bankinter", "Van Ginder", "Banquinter", "Bunkinter", "Bank inter")
)
```

```{r, cache=TRUE}
# clean the sentences which repeat the question
# to detect correctly 
clean_awareness_text <- function(text) {
  gsub(
    pattern = "(?i)(los bancos que más.*?(son|serían|pueden ser|diría que|han sido|yo creo que|suelo escuchar|en mi día a día)[,\\s]*)",
    replacement = "",
    x = text,
    perl = TRUE
  )
}

# create the function to generate the Awareness Index（1–7）
compute_awareness_score_scaled <- function(text, assigned_bank, bank_aliases) {
  if (is.na(text) || is.na(assigned_bank)) return(NA)

  # Step 1: clean the sentences
  cleaned_text <- clean_awareness_text(text)

  # Step 2: tokenization
  tokens <- unlist(strsplit(cleaned_text, ",|\\s+"))
  tokens <- tolower(trimws(tokens))

  # Step 3: fuzzy assignation of brands
  aliases <- tolower(bank_aliases[[assigned_bank]])
  match_scores <- sapply(tokens, function(tok) {
    min(stringdist(tok, aliases, method = "jw"))
  })

  threshold <- 0.2
  hit_positions <- which(match_scores < threshold)

  # Step 4: 1 for not mentioning the assigned brand
  if (length(hit_positions) == 0) return(1)

  # Step 5: find the firstly mentioned brand (and the first 5)
  pos <- hit_positions[1]
  if (pos > 5) pos <- 5

  # Step 6: scaling to 1-7（pos 1–5 → 1–7）
  raw_score <- max(0, 1 - 0.2 * (pos - 1))  # e.g. 1.0 → 0.8 → ... → 0.2
  scaled_score <- 1 + 6 * raw_score        # scaling to 1–7
  return(round(scaled_score))              # round the number to integers in the Likert scale
}
```

```{r, cache=TRUE}
tidy_join_transcript1 <- tidy_join_transcript  |> 
  rowwise()  |> 
  mutate(
    awareness_index_base = compute_awareness_score_scaled(
      awareness_audio,
      brand,
      bank_aliases
    )
  )  |> 
  ungroup()  
```

### Sentiment index

I'll use text mining tools to conduct the sentiment analysis, but first
of all, I need to download a Spanish sentiment lexicon as R libraries
doesn't have it in Spanish. The lexicon dictionary I use is from
IULA-UPF ([Institut de Lingüística Aplicada -
Barcelona](https://www.upf.edu/web/iula)), and the link to download the
lexicon is
<https://github.com/ITALIC-US/ML-Senticon/blob/main/senticon.es.xml>.

```{r, cache=TRUE}
xml_file <- read_xml("data/lexicons/senticon.es.xml")
lemmas <- xml_find_all(xml_file, ".//lemma")
senticon_es <- map_dfr(lemmas, function(node) {
  word <- xml_text(node) %>% str_trim()
  polarity <- xml_attr(node, "pol") %>% as.numeric()

  tibble(word = str_to_lower(word), polarity = polarity)
}) %>%
  filter(!is.na(polarity)) %>%
  distinct(word, .keep_all = TRUE)
```

I'll use `spacyr` to tokenize and lemmatize the transcripts (main audio)
in Spanish:

```{bash}
# pip install spacy
# python -m spacy download es_core_news_md
```

```{r}
if (!dir.exists("data/intermediate")) dir.create("data/intermediate", recursive = TRUE)
readr::write_csv(tidy_join_transcript1, "data/intermediate/tidy_join_transcript1.csv")
```

```{python, cache=TRUE}
import spacy
import pandas as pd

# model the Spanish model
nlp = spacy.load("es_core_news_md")


df = pd.read_csv("data/intermediate/tidy_join_transcript1.csv")

results = []

for idx, row in df.iterrows():
    respondent_id = row["respondent_id"]
    doc = nlp(str(row["main_audio"]))
    for token in doc:
        if not token.is_punct and not token.is_space:
            results.append({
                "respondent_id": respondent_id,
                "lemma": token.lemma_.lower()
            })

# save as dataframe
lemmas_df = pd.DataFrame(results)
lemmas_df.to_csv("data/intermediate/lemmas.csv", index=False)
```

Then, I apply the lexicons dictionary to the transcripts by
`inner_join`:

```{r, cache=TRUE}
tokens_lemma <- readr::read_csv("data/intermediate/lemmas.csv")
sentiment_scores <- tokens_lemma %>%
  inner_join(senticon_es, by = c("lemma" = "word")) %>%
  group_by(respondent_id) %>%
  summarise(sentiment_raw = mean(polarity, na.rm = TRUE)) %>%
  mutate(sentiment_index = round(scales::rescale(sentiment_raw, to = c(1, 7))))

```

Finally, I join the sentiment scores to the original tibble.

```{r, cache=TRUE}
tidy_join_transcript2 <- tidy_join_transcript1 |> 
  left_join(sentiment_scores, by="respondent_id") |> 
  select(-"sentiment_raw") |> 
  rename(sentiment_index_base=sentiment_index)
```

### Differentiation index

To compute the Differentiation Index, I employed a semantic similarity
approach using a multilingual BERT model. As reference prompts, I
designed a set of short, general-purpose sentences that semantically
express the concept of brand distinctiveness without assuming any
specific source of differentiation (e.g., technology, customer focus,
pricing). This design choice ensures compatibility with a wide range of
respondent expressions while maintaining methodological neutrality.
Example prompts include: “Este banco es diferente a los demás.”, “Tiene
algo único que lo distingue del resto.”, and “No es un banco
convencional.” These prompts were embedded using Sentence-BERT and used
to compute cosine similarity with each respondent’s free-form
description of the assigned brand.

Firstly, I install the `sentence-transformers` python package.

```{bash, cache=TRUE}
# pip install sentence-transformers
```

Then, I prepared the input and the prompts for the BERT model:

```{r, cache=TRUE}
readr::write_csv(tidy_join_transcript2, "data/intermediate/tidy_join_transcript2.csv")
```

```{python, eval=FALSE, echo=TRUE}
# # bert_differentiation.py
# 
# from sentence_transformers import SentenceTransformer, util
# import pandas as pd
# import numpy as np
# 
# # 1. Load the input CSV from R
# df = pd.read_csv("data/intermediate/tidy_join_transcript2.csv")  # adapt path if needed
# texts = df["main_audio"].astype(str).tolist()
# 
# # 2. Define general-purpose differentiation prompts (in Spanish)
# prompts = [
#     "Este banco es diferente a los demás.",
#     "Tiene algo único que lo distingue del resto.",
#     "Se nota que no es como otros bancos.",
#     "Ofrece una propuesta distinta a lo habitual.",
#     "No es un banco convencional.",
#     "Tiene una manera propia de hacer las cosas.",
#     "Es claramente un banco con enfoque diferente."
# ]
# 
# # 3. Load the multilingual BERT model
# model = SentenceTransformer("distiluse-base-multilingual-cased-v2")
# 
# # 4. Encode texts and prompts
# text_embeddings = model.encode(texts, convert_to_tensor=True)
# prompt_embeddings = model.encode(prompts, convert_to_tensor=True)
# 
# # 5. Compute cosine similarity
# similarity_matrix = util.cos_sim(text_embeddings, prompt_embeddings).cpu().numpy()
# 
# # 6. Take the maximum similarity score for each respondent
# max_similarities = np.max(similarity_matrix, axis=1)
# 
# # 7. Rescale to 1–7 differentiation index
# scaled_scores = 1 + 6 * (max_similarities - np.min(max_similarities)) / (np.max(max_similarities) - np.min(max_similarities))
# df["differentiation_index"] = np.round(scaled_scores)
# 
# # 8. Save output for R
# df[["respondent_id", "differentiation_index"]].to_csv("data/intermediate/differentiation_index.csv", index=False)
# 
# print("✅ Differentiation index saved to data/intermediate/differentiation_index.csv")
```

As this model is computationally costly, and the R studio burnt out
several times while runing the codes above, I will not run this part of
Python code here, but to save it as .py and run it in the Terminal:

```{bash}
# conda create -n tfm-bert python=3.10
# conda activate tfm-bert
# pip install sentence-transformers pandas numpy
# cd ~/Desktop/MCSC/TFM/Project/Feature-Extraction-and-Brand-Index-Creation-from-Semi-Structured-Spoken-Responses-using-LLMs-and-ML-tools
# python data/intermediate/bert_differentiation.py
```

```{r, cache=TRUE}
# Join the scores to the main tibble
diff_scores <- readr::read_csv("data/intermediate/differentiation_index.csv")

tidy_join_transcript3 <- tidy_join_transcript2 %>%
  left_join(diff_scores, by = "respondent_id") |> 
  rename(differentiation_index_base=differentiation_index)

readr::write_csv(tidy_join_transcript3, "data/intermediate/tidy_join_transcript3.csv")
```

### Performance evaluation

To evaluate the performance of the proposed index predictions—derived
from both rule-based methods and LLM prompt outputs—against
human-annotated scores on a 1–7 Likert scale, we selected **three
complementary evaluation metrics**: **Mean Absolute Error (MAE)**,
**Spearman’s rank correlation coefficient (ρ)**, and **1-off Accuracy**.
Each metric captures a different and relevant dimension of prediction
quality in the context of ordinal, human-rated scores.

**1. Mean Absolute Error (MAE)**

MAE was chosen as the **primary metric for measuring prediction
accuracy**. It quantifies the average absolute deviation between model
predictions and human ratings.

This metric is:

-   **Scale-aware**, making it suitable for Likert-type ratings (1–7);

-   **Interpretation-friendly**, as it reflects how many points the
    model typically deviates from the ground truth;

-   **Robust to outliers** compared to squared-error metrics such as
    MSE, which are less appropriate for short, bounded rating scales.

Given the limited sample size (n = 29), MAE provides a **stable and
intuitive measure of absolute prediction quality**.

**2. Spearman’s Rank Correlation Coefficient (ρ)**

Spearman’s ρ was selected to assess the **ordinal consistency** between
predicted scores and human annotations. As a non-parametric rank-based
metric, it evaluates whether the predicted scores preserve the
**relative ordering** of responses, independent of their exact values.

This is especially relevant because:

-   Likert scales are **ordinal by nature**, and rank alignment may
    matter more than exact numerical matching;

-   It is **less sensitive to score compression or skew**—common in
    subjective human ratings;

-   It is particularly **appropriate for small samples**, where
    assumptions of linearity or normality (required by Pearson’s r) may
    not hold.

Thus, Spearman ρ serves as a complementary metric to MAE, providing
insights into **directional alignment** between models and humans.

**3. 1-off Accuracy**

This metric captures the **practical tolerance** of the prediction task
by measuring the proportion of predictions that fall within ±1 point of
the true human rating.

Its inclusion reflects:

-   The **realistic fuzziness** inherent in subjective annotations;

-   The fact that a predicted 5 vs an annotated 6 is **not a critical
    error** in most qualitative applications;

-   The need to quantify the model’s **"acceptable correctness"** beyond
    rigid exact-match accuracy.

1-off Accuracy offers an interpretable, **task-relevant performance
measure**, especially for user-perception data where perfect alignment
is neither expected nor necessary.

```{r, cache=TRUE}

# Create a function to standarize the evaluation
evaluate_model <- function(pred, truth) {
  mae <- mean(abs(pred - truth), na.rm = TRUE)
  spearman <- suppressWarnings(cor(pred, truth, method = "spearman", use = "pairwise.complete.obs"))
  one_off <- mean(abs(pred - truth) <= 1, na.rm = TRUE)

  return(c(
    MAE = round(mae, 3),
    Spearman = round(spearman, 3),
    OneOffAcc = round(one_off * 100, 1)  
  ))
}

results_base <- tibble::tibble(
  Index = character(),
  MAE = numeric(),
  Spearman = numeric(),
  OneOffAcc = numeric()
)

indexes <- c("awareness", "sentiment", "differentiation")

# go through all indexes, and find their _index_base 
for (ix in indexes) {
  pred_col <- paste0(ix, "_index_base")
  truth_col <- ix  
  
  eval_result <- evaluate_model(tidy_join_transcript3[[pred_col]], tidy_join_transcript3[[truth_col]])
  
  results_base <- dplyr::add_row(results_base, Index = ix, !!!as.list(eval_result))
}

print(results_base)

```

The performance of the baseline models across the three brand perception
dimensions is limited:

-   **Awareness**:\
    High prediction error (MAE = 2.90), weak rank correlation (ρ =
    0.21), and very low 1-off accuracy (31.6%) suggest poor alignment
    with human ratings. The rule-based method likely lacks precision in
    detecting brand mentions and ordering relevance.

-   **Sentiment**:
    Moderate performance (MAE = 1.37, ρ = 0.17, 1-off = 60.5%). While
    not highly accurate, this model captures some degree of emotional
    polarity and could serve as a viable baseline for future LLM
    comparisons.

-   **Differentiation**:\
    Poor performance, with negative rank correlation (ρ = –0.51) and
    high MAE (1.90). This suggests that the semantic similarity method
    used in the BERT-based baseline may not effectively capture
    perceived brand uniqueness.

> Among the three dimensions, only the sentiment baseline shows
> marginally acceptable performance. Awareness and differentiation
> models will likely require redesign or replacement using more robust
> methods (e.g. LLMs or hybrid pipelines).

## Selection of LLM for Evaluation

In order to evaluate the effectiveness of automatic analysis on
Spanish-language audio transcriptions, we selected three distinct Large
Language Models (LLMs) based on their current performance in academic
benchmarks, multilingual capabilities, cost-efficiency, and
architectural diversity. The selection is grounded in recent comparative
research and official technical documentation.

First, **GPT-4o**, developed by OpenAI, was chosen as the primary model.
It has consistently ranked at the top of evaluations such as the Chatbot
Arena and the MT-Bench for its superior capabilities in complex
reasoning, structured output, and instruction following. Most notably,
it demonstrates strong multilingual performance, including for Spanish,
as reported in the GPT-4o System Card published by OpenAI (2024).
Moreover, recent studies indicate that GPT-4o displays notable
robustness to prompt variation, minimizing the need for excessive prompt
engineering in practical applications (Zhao et al., 2023). These
attributes make GPT-4o a strong candidate for our baseline model.

To introduce architectural diversity and cross-validation, we also
included **Claude 3 Sonnet**, developed by Anthropic. This model has
emerged as a serious contender to GPT-4o, with highly competitive
results in structured response generation tasks. It is particularly
suited for evaluations requiring consistent JSON-formatted outputs, a
core requirement in this research. Furthermore, Claude 3 Sonnet offers a
balanced trade-off between quality and cost, making it especially
suitable for batch processing in research environments. Technical
documentation from Anthropic (2024) affirms its capabilities in both
English and Spanish, supporting its relevance to our multilingual
dataset.

The third model incorporated into the evaluation is **Mistral 7B**, a
leading open-source alternative. Despite its smaller size, Mistral 7B
has outperformed earlier open-source models such as LLaMA 2 in zero-shot
and few-shot benchmarks. Running this model locally not only ensures
cost control—eliminating per-token API expenses—but also enhances
transparency and reproducibility. Including Mistral 7B also allows us to
investigate whether closed-source commercial models consistently
outperform open alternatives in tasks involving informal spoken Spanish,
a consideration highlighted in recent comparative studies (Zhao et al.,
2024).

Other models, such as **Google Gemini 1.5 Pro**, were considered but
ultimately excluded due to limited evidence of superior Spanish language
performance. Additionally, models like **Cohere's Command R+**, while
excellent in retrieval-augmented generation (RAG), are not optimized for
subjective classification tasks without retrieval support. Lastly,
**LLaMA 2-7B** was excluded based on its substantially lower benchmark
performance across reasoning tasks when compared to both GPT-4o and
Claude 3.

This tripartite model selection—combining a top-tier commercial model
(GPT-4o), a competitive alternative from a different provider (Claude 3
Sonnet), and a state-of-the-art open-source model (Mistral 7B)—ensures
that our evaluation captures not only accuracy and robustness but also
cost considerations and implementation feasibility in both academic and
applied settings.

## References

Liang, P., et al. (2024). *Chatbot Arena and LMSYS Leaderboard.*
Retrieved from <https://chat.lmsys.org>\

OpenAI. (2024). *GPT-4o System Card.* Retrieved from
<https://openai.com/research/gpt-4o-system-card>\

Zhao, W., et al. (2023). *Prompt Robustness in LLMs: An Empirical
Study.* arXiv:2309.01234\

Anthropic. (2024). *Claude 3 Technical Report.* Retrieved from
<https://www.anthropic.com/news/claude-3-family>\

Anthropic. (2025). *Claude 3 Pricing.* Retrieved from
<https://www.anthropic.com/pricing>\

Mistral AI. (2023). *Introducing Mistral 7B.* Retrieved from
<https://mistral.ai/news/mistral-7b/>\

Touvron, H., et al. (2023). *LLaMA 2: Open Foundation and Fine-Tuned
Chat Models.* arXiv:2307.09288\

Zhao, Z., et al. (2024). *Are Open-Source LLMs Catching Up? A
Comparative Study.* arXiv:2402.03211\

Cohere. (2024). *Command R+ Overview.* Retrieved from
<https://cohere.com>
